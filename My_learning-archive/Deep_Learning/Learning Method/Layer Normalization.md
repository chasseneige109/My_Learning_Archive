
### 1. ğŸ“ í•µì‹¬ ê³„ì‚° ìˆ˜ì‹

ì…ë ¥ í–‰ë ¬ $X \in \mathbb{R}^{N \times D}$ì—ì„œ, Layer Normì€ 'ê° ìƒ˜í”Œ $i$'ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ í†µê³„ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

#### 1) í‰ê·  $\mu_i$ ë° ë¶„ì‚° $\sigma_i^2$ (ìƒ˜í”Œ $i$ì— ëŒ€í•´)

$$\mu_i = \frac{1}{D} \sum_{j=1}^{D} x_{i j}$$

$$\sigma_i^2 = \frac{1}{D} \sum_{j=1}^{D} (x_{i j} - \mu_i)^2$$

#### 2) ì •ê·œí™”ëœ ì¶œë ¥ $\hat{x}_{i j}$

$$\hat{x}_{i j} = \frac{x_{i j} - \mu_i}{\sqrt{\sigma_i^2 + \epsilon}}$$

---

### 2. ğŸšï¸ ì–´íŒŒì¸ ë³€í™˜ (Affine Transformation)

ì •ê·œí™”ëœ $\hat{X}$ì— **í”¼ì²˜ë³„ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ** ìŠ¤ì¼€ì¼ $\boldsymbol{\gamma}$ì™€ ì´ë™ $\boldsymbol{\beta}$ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ($\boldsymbol{\gamma}, \boldsymbol{\beta} \in \mathbb{R}^{D}$)

$$y_{i j} = \gamma_j \hat{x}_{i j} + \beta_j$$

- $\boldsymbol{\gamma}$ì™€ $\boldsymbol{\beta}$ëŠ” **í”¼ì²˜ ì°¨ì›**ì— ëŒ€í•´ì„œë§Œ í•™ìŠµë©ë‹ˆë‹¤.
    

---

### 3. âš–ï¸ Batch Normê³¼ì˜ ë¹„êµ

| **êµ¬ë¶„**       | **Layer Normalization (Layer Norm)** | **Batch Normalization (Batch Norm)** |
| ------------ | ------------------------------------ | ------------------------------------ |
| **í†µê³„ëŸ‰ ê³„ì‚° ì¶•** | **í”¼ì²˜ ì°¨ì› ($D$)** $\rightarrow$ (ìƒ˜í”Œ ë‚´) | **ë°°ì¹˜ ì°¨ì› ($N$)** $\downarrow$ (í”¼ì²˜ ê°„)  |
| **ì˜ì¡´ì„±**      | **ë°°ì¹˜ í¬ê¸°ì— ë…ë¦½ì **                       | ë°°ì¹˜ í¬ê¸°ì— ì˜ì¡´ì  (ë°°ì¹˜ê°€ ì‘ìœ¼ë©´ ì„±ëŠ¥ ì €í•˜)           |
| **ì£¼ìš” ì‚¬ìš©ì²˜**   | **RNN, Transformer** (ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤)     | CNN, Fully Connected Layer           |