**`[CLS]` 토큰 (Classification Token)**은 Vision Transformer(ViT)에서 **전체 이미지를 대표하는 요약 정보**를 담는 특수 토큰입니다.

쉽게 비유하자면, **"반장"**이나 **"빈 요약 노트"**

---

### 1. 역할: "정보를 수집하는 빈 그릇"

- **이미지 패치들(0~9번):** 얘네들은 자기 구역(패치)의 정보만 가지고 시작합니다. ("난 왼쪽 위 모서리야", "난 고양이 귀 부분이야")
    
- **`[CLS]` 토큰:** 얘는 **아무런 이미지 정보 없이** 시작합니다. (학습 가능한 랜덤 벡터로 초기화됨)
    

하지만 트랜스포머 층(Layer)을 통과하면서 **Self-Attention**이 일어납니다. 이때 `[CLS]` 토큰은 모든 패치들을 둘러보며 **"중요한 정보"를 자기 자신에게 흡수**합니다.

> _"어? 3번 패치에 귀가 있고, 5번 패치에 수염이 있네? 그럼 이건 고양이구나."_

### 2. 작동 과정: 시작부터 끝까지

1. **입력 단계:** 맨 앞에 **`[CLS]`** 토큰을 붙여서 $(N+1)$개의 시퀀스를 만듭니다.
    
2. **인코딩 단계:** 층을 거듭할수록 `[CLS]` 토큰은 다른 모든 패치($1 \sim N$)와 상호작용(Attention)하며, **이미지 전체의 문맥(Global Context)**을 업데이트합니다.
    
3. **출력 단계:** 마지막 층에서 나온 $(N+1)$개의 벡터 중, 나머지 패치 벡터들은 다 버리고 **오직 `[CLS]` 토큰의 벡터 하나만** 쏙 뽑아서 **MLP Head(분류기)**에 넣습니다.
    

### 3. 왜 굳이 이걸 쓰나요? (vs Global Average Pooling)

CNN에서는 보통 마지막에 모든 특징맵의 평균을 구하는 **Global Average Pooling (GAP)**을 씁니다. ViT에서도 패치들의 평균을 써도 되지만, `[CLS]` 토큰을 쓰는 이유는 **BERT(NLP 모델)**의 설계를 그대로 계승했기 때문입니다.

- **학습의 목적성:** 모델에게 *"야, 나중에 이 **맨 앞자리(`[CLS]`)*_만 보고 정답 맞힐 거니까, 여기에 정보를 잘 압축해 놔!"_ 라고 강제하는 효과가 있습니다.
    
- **유연성:** 평균을 내면 정보가 희석될 수 있지만, `[CLS]` 토큰은 신경망이 학습을 통해 **"어떤 정보를 남기고 어떤 정보를 버릴지"** 더 정교하게 선택할 수 있습니다.
    

요약하자면:

[CLS] 토큰은 이미지를 잘게 쪼갠 패치들의 정보를 하나로 모아 **"이건 고양이다"*라는 최종 결론을 내리기 위해 존재하는 대표 선수입니다. 사용자님이 보내주신 그림에서 MLP Head로 들어가는 화살표가 오직 [CLS] 토큰(0)에서만 나오는 이유가 바로 이것입니다.