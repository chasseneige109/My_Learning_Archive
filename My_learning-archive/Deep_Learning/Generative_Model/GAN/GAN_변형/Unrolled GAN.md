**Unrolled GAN**은 GAN의 고질적인 문제인 '불안정한 학습'과 'Mode Collapse'를 해결하기 위해, **Generator에게 "미래를 내다보는 능력(Lookahead)"을 부여한 기법**입니다.

마치 체스나 바둑 고수가 "내가 이렇게 두면 상대방은 저렇게 둘 것이고..."라고 몇 수 앞을 내다보고 현재의 수를 두는 것과 정확히 같은 원리입니다.

---

### 1. 핵심 아이디어: "근시안(Greedy)" vs "천리안(Unrolled)"

#### 기존 GAN (근시안적 접근)

- **상황:** Generator($G$)는 **현재 상태의 Discriminator($D$)** 만을 속이려 합니다.
    
- **행동:** $D$가 현재 '숫자 1'을 잘 못 알아본다면, $G$는 당장 점수를 따기 위해 모든 데이터를 '1'로 만들어버립니다. (Mode Collapse)
    
- **결과:** $D$가 다음 턴에 학습해서 '1'을 막으면, $G$는 다시 '8'로 도망갑니다. 서로 꼬리 잡기만 하느라 분포를 배우지 못하고 뱅글뱅글 돕니다.
    

#### Unrolled GAN (천리안적 접근)

- **상황:** $G$는 **"내가 이렇게 변하면, $D$가 어떻게 대응해서 똑똑해질까?"** 를 시뮬레이션해 봅니다.
    
- **행동:**
    
    1. 가상으로 $D$를 $k$번 학습시켜 봅니다 (Unrolling).
        
    2. $k$번 업그레이드된 미래의 $D^k$를 상대로도 점수를 잘 받을 수 있는 위치를 찾습니다.
        
- **결과:** "지금 당장 '1'만 만들면 점수는 좋겠지만, 5스텝 뒤에 $D$가 '1'을 완벽히 차단할 거야. 그러니까 '1'만 만들지 말고 골고루 만들어서 미래의 $D$에게도 살아남아야지." $\rightarrow$ **Mode Collapse 방지**
    

---

### 2. 작동 메커니즘 (Step-by-Step)

이 과정은 **Meta-Learning**과 매우 유사합니다.

1. **현재 상태:** $G$의 파라미터 $\theta_G$, $D$의 파라미터 $\theta_D$.
    
2. **가상 시뮬레이션 (Unrolling):**
    
    - $\theta_G$는 고정시킨 채, 복사본 $D$를 가지고 학습 루프를 $k$번(예: 5~10번) 돌려봅니다.
        
    - $\theta_D \rightarrow \theta_D^1 \rightarrow \theta_D^2 \dots \rightarrow \theta_D^k$ (미래의 최적화된 $D$)
        
    - 여기서 $\theta_D^k$는 **현재의 $\theta_G$에 대한 함수**가 됩니다. (G가 다르면 D의 대응도 달라지므로)
        
3. **Generator 업데이트:**
    
    - $G$의 목표는 **현재의 $D$가 아니라, 미래의 $D^k$를 속이는 것**입니다.
        
    - Loss 함수: $L_G(\theta_G, \theta_D^k)$
        
    - 이 Loss를 줄이는 방향으로 실제 $\theta_G$를 업데이트합니다.
        
4. **Discriminator 업데이트:**
    
    - $D$는 원래대로 한 스텝 업데이트합니다.
        

---

### 3. 왜 이게 Mode Collapse를 막는가?

수학적으로 $D$가 최적화($k \to \infty$)될수록, $D$의 목적 함수는 $G$가 만든 분포와 실제 분포 사이의 **JS Divergence**를 정확하게 근사하게 됩니다.

- **기존 GAN:** 덜 학습된 $D$는 완벽한 JSD가 아니므로, $G$가 꼼수(Mode Collapse)를 부릴 구멍이 있습니다.
    
- **Unrolled GAN:** $G$가 "미래의 똑똑해진 $D$"를 상대한다는 것은, 사실상 **$D$가 꼼수를 다 파악하고 있는 상태(JSD가 정확히 측정되는 상태)**를 가정하는 것입니다.
    
    - 미래의 $D$는 $G$가 한 곳에 뭉쳐 있으면(Collapse) 가차 없이 그 부분을 가짜라고 판별할 것입니다.
        
    - 이 미래를 피하기 위해 $G$는 **현재 시점부터 데이터를 넓게 펼치는(Spread out) 전략**을 선택하게 됩니다.
        

---

### 4. 치명적인 단점: 계산 비용 (Gradient of Gradient)

이 방법이 이론적으로는 훌륭하지만, 널리 쓰이지 못하는 이유는 **살인적인 계산 비용** 때문입니다.

$G$를 업데이트하기 위해 미분을 할 때 Chain Rule을 적용해야 합니다.

$$\frac{d L}{d \theta_G} = \frac{\partial L}{\partial \theta_G} + \frac{\partial L}{\partial \theta_D^k} \cdot \frac{\partial \theta_D^k}{\partial \theta_D^{k-1}} \cdots \frac{\partial \theta_D^1}{\partial \theta_G}$$

1. **메모리 폭발:** $k$번의 $D$ 업데이트 과정을 모두 **Computational Graph(연산 그래프)**에 저장해야 합니다. Backpropagation을 하려면 중간 단계의 값들이 다 필요하기 때문입니다.
    
2. **시간 폭발:** $G$를 한 번 업데이트할 때마다 $D$를 $k$번 미분하는 연산을 수행해야 합니다. (이중 루프 구조)
    
3. **Gradient Vanishing/Exploding:** 그래프가 $k$배 깊어지는 셈이라, RNN처럼 기울기 전달이 어려워질 수 있습니다.
    

### 요약

- **Unrolled GAN:** $G$가 "내가 이렇게 움직이면 $D$가 어떻게 반응할까?"를 시뮬레이션($k$ steps)해서, $D$의 반응 후에도 살아남을 수 있는 최적의 위치로 이동함.
    
- **효과:** 근시안적인 꼼수(Mode Collapse)가 미래의 $D$에게는 통하지 않음을 알기에, $G$가 스스로 다양성을 확보함.
    
- **대가:** $k$스텝만큼의 연산 그래프를 뚫고 미분해야 하므로 **메모리와 계산 시간이 엄청나게 소요됨.**