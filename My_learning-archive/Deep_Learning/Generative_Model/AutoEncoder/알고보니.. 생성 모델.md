**"분류 모델(Discriminative Model)의 내부를 뜯어보니, 사실상 생성 모델(Generative Model)의 부품을 가지고 있더라"**는 딥러닝의 심오한 통찰을 다루고 있습니다.

특히 **"가중치의 전치행렬($W^T$)로 합성한다"**는 개념은 선형대수학의 **기저(Basis)와 투영(Projection)** 개념으로 완벽하게 설명됩니다. 이를 수학적으로 해부해 드리겠습니다.

---

### 1. 분류 네트워크의 은닉층: "특징 감지기" (Feature Detector)

숫자(Digit) 분류기 학습이 완료된 첫 번째 은닉층을 생각해 봅시다.

입력 벡터 $x \in \mathbb{R}^{784}$ ($28 \times 28$ 이미지)가 있고, 은닉층 가중치 $W \in \mathbb{R}^{K \times 784}$가 있습니다. ($K$는 뉴런 개수)

은닉층의 출력(Activation) $h$는 다음과 같습니다.

$$h = \sigma(W x)$$

이 행렬 곱 $Wx$를 행(Row) 단위로 뜯어보면 의미가 명확해집니다.

$W$의 $i$번째 행 벡터를 $w_i$라고 할 때 ($w_i$는 784차원, 즉 이미지와 같은 차원):

$$h_i = \sigma(w_i \cdot x)$$

- **내적(Dot Product)의 의미:** $w_i \cdot x$는 두 벡터의 **유사도(Similarity)**를 측정합니다.
    
- **해석:** $w_i$가 "수평선 패턴"을 가진 벡터라면, 입력 $x$에 수평선이 있을 때 내적 값이 커지고, 뉴런 $h_i$가 활성화됩니다.
    
- **결론:** **가중치 벡터 $w_i$들**은 이미지 공간 상에서 의미 있는 **"특징 템플릿(Feature Template)"** 또는 **"기저 벡터(Basis Vector)"** 역할을 합니다.
    

---

### 2. 가중치 Transpose로 합성: "재구성" (Reconstruction)

이제 이 분류기의 가중치 $W$를 그대로 사용하여, 거꾸로 이미지를 만들어본다고 가정해 봅시다. 이를 **Tied Weights** (인코더와 디코더 가중치를 공유) 개념이라고도 합니다.

복원된 이미지 $\hat{x}$를 만들기 위해 $h$와 $W^T$를 곱합니다.

$$\hat{x} = W^T h$$

이 수식을 선형대수학의 선형 결합(Linear Combination) 관점에서 풀면 놀라운 직관이 나옵니다.

$W^T$의 열(Column) 벡터는 원래 $W$의 행(Row) 벡터인 $w_i$와 같습니다.

$$\hat{x} = \sum_{i=1}^{K} h_i \cdot w_i$$

- **수식 해석:**
    
    - $w_i$: 특징 템플릿 (예: 수평선 그림, 동그라미 그림)
        
    - $h_i$: 그 특징이 얼마나 강한지 나타내는 계수 (Scalar)
        
    - $\sum$: 이들을 가중 합산하여 이미지를 합성.
        
- **직관:** "수평선($w_1$)이 0.8만큼 있고, 동그라미($w_2$)가 0.1만큼 있으니, 그 그림들을 겹쳐서 원본을 대략 복구해라."
    

---

### 3. "Class-relevant"의 의미: 분류기는 편식쟁이다

문제는 이 $W$가 **"분류(Classification)"**를 위해 학습되었다는 점입니다.

- **목적 함수(Loss):** Cross-Entropy (정답 숫자만 맞추면 됨).
    
- **결과:** 분류기는 **"숫자를 구별하는 데 도움이 되는 특징"**만 학습합니다.
    
    - 숫자 1과 7을 구별하는 데 '기울기'나 '가로 획'은 중요하므로 $w_i$에 저장됩니다.
        
    - 하지만 '배경 색깔', '붓글씨의 질감', '노이즈' 등은 분류에 방해되거나 쓸모없으므로 **무시(0으로 만듦)**합니다.
        
- 재구성 결과:
    
    이렇게 학습된 $W^T$로 $\hat{x}$를 만들면, 원본의 디테일은 다 날아가고 **"숫자라는 것을 알아볼 수 있는 뼈대(Class-relevant features)"**만 남은 유령 같은 이미지가 복원됩니다.
    

---

### 4. Autoencoder: "편식하지 말고 다 복원해"

Autoencoder는 이 메커니즘을 **"입력 전체를 복원하는 것"**으로 일반화한 것입니다.

- **목적 함수(Loss):** MSE ($\|x - \hat{x}\|^2$). 입력과 똑같이 만들어야 함.
    
- **차이점:**
    
    - Autoencoder의 가중치 $W$는 숫자를 구별하는 특징뿐만 아니라, **이미지의 두께, 스타일, 배경 노이즈 등 모든 정보를 표현할 수 있는 기저 벡터(Basis Vectors)**를 학습해야 합니다.
        
    - 즉, 데이터가 존재하는 **전체 다양체(Manifold)**를 아우르는 기저(Basis)를 찾는 과정입니다 (PCA의 비선형 확장).
        

### 요약: 행렬 관점에서의 재구성

1. **Forward ($Wx$):** 입력 이미지를 기저 벡터($w_i$)들에 **투영(Projection)**하여 "성분 값($h_i$)"을 추출합니다. (분석 단계)
    
2. **Backward ($W^Th$):** 추출된 성분 값($h_i$)을 계수로 사용하여, 다시 기저 벡터($w_i$)들을 **선형 결합(Linear Combination)**하여 이미지를 합성합니다. (합성 단계)
    

**"분류기는 정답을 맞추기 위해 필요한 최소한의 기저 벡터만 챙기지만, Autoencoder는 원본을 완벽히 그리기 위해 필요한 모든 기저 벡터를 챙긴다"**는 것이 핵심 차이입니다.