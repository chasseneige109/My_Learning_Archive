



## MLE
![[Pasted image 20251213215457.png]]

![[Pasted image 20251213220152.png]]



$$\log P(x) = \underbrace{\mathbb{E}_{Q}[\log P(x|z)]}_{\text{1번 항}} - \underbrace{D_{KL}(Q(z|x) \| P(z))}_{\text{2번 항}} + \underbrace{D_{KL}(Q(z|x) \| P(z|x))}_{\text{3번 항}}$$

질문자님의 해석을 하나씩 짚어보겠습니다.

---

### **1. 첫 번째 항: Reconstruction Term**

$$\mathbb{E}_{z \sim Q_\phi(z|x)}[\log P_\theta(x|z)]$$

- **질문자님 해석:** "eps = N(0,1)로 np.random해서 뽑는 trick을 써서... 샘플을 decoder에 넣는 방식으로 구한다."
    
- **팩트 체크:** **정확합니다.**
    
- **상세 설명:**
    
    - 이 항은 기댓값($\mathbb{E}$)입니다. 기댓값을 수학적으로 정확히 계산하려면 모든 $z$에 대해 적분해야 하는데 불가능하죠.
        
    - 그래서 **Monte Carlo Approximation(몬테카를로 근사)**을 씁니다. 즉, **"딱 1번만 샘플링($z$)"** 해서 나온 결과가 평균을 대변한다고 퉁치는 것입니다.
        
    - **과정:**
        
        1. Encoder에서 $\mu, \sigma$ 나옴.
            
        2. `z = mu + sigma * eps` (Reparameterization Trick).
            
        3. Decoder에 `z`를 넣음 $\to$ `output` 나옴.
            
        4. `output`과 원본 `x` 사이의 오차(MSE or BCE) 계산.
            
    - 이 과정 자체가 수식의 값을 계산하는 과정이 됩니다.
        

### **2. 두 번째 항: Prior KL Divergence**

$$D_{KL}(Q_\phi(z|x) \| P(z))$$

- **질문자님 해석:** "둘 다 보통 Gaussian으로 가정하니까 쉽게 구할 수 있다."
    
- **팩트 체크:** **정확합니다.**
    
- **상세 설명:**
    
    - $Q_\phi(z|x)$: Encoder가 뱉어낸 정규분포 $\mathcal{N}(\mu, \sigma^2)$.
        
    - $P(z)$: 우리가 고정한 표준정규분포 $\mathcal{N}(0, 1)$.
        
    - **가우시안 대 가우시안의 KL Divergence**는 적분할 필요 없이, 평균과 분산만 알면 바로 답이 나오는 **공식(Closed-form solution)**이 존재합니다.
        
    - 우리가 코딩할 때 `sampling` 없이 $\mu$와 $\sigma$ 값만 가지고 바로 Loss를 계산하는 이유입니다.
        

### **3. 세 번째 항: Posterior KL Divergence**

$$D_{KL}(Q_\phi(z|x) \| P(z|x))$$

- **질문자님 해석:** "우리가 p(z|x)를 알 수가 없기 때문에 계산하는 것이 불가능하다."
    
- **팩트 체크:** **이것이 핵심입니다. 정확합니다.**
    
- **상세 설명:**
    
    - 여기서 $P(z|x)$는 **True Posterior(진짜 사후 확률)**입니다.
        
    - $P(z|x) = \frac{P(x|z)P(z)}{P(x)}$ 인데, 분모인 $P(x)$(Evidence)를 구하려면 모든 $z$에 대해 적분해야 하므로 계산이 불가능(Intractable)합니다.
        
    - **그래서 VAE의 전략은:**
        
        - "3번 항은 양수($\ge 0$)인 건 아는데, 값은 모른다."
            
        - "그럼 3번 항을 떼어버리고, **1번 항 - 2번 항 (ELBO)**만이라도 최대화하자."
            
        - "ELBO가 커지면 전체 $\log P(x)$도 커질 것이고, 3번 항(근사 오차)도 자연스럽게 줄어들 것이다."







## 수학적 의의

#### **A. "점"이 아니라 "영역"으로 매핑한다**

- **일반 AE:** 입력 이미지를 잠재 공간의 좌표 $(2.5, -1.3)$ 같은 **점**으로 보냅니다.
    
- **VAE:** 입력 이미지를 $(2.5, -1.3)$을 중심으로 하고 반지름(분산)이 있는 **구름(분포)**으로 보냅니다.
    
    - Encoder는 "이 이미지는 대략 이 근처 쯤에 있어"라고 **불확실성(Uncertainty)**을 포함해서 알려주는 것입니다.
        

#### **B. 두 가지 힘의 줄다리기 (Trade-off)**

Loss Function의 두 항은 서로 상충되는 목적을 가집니다.

1. **Reconstruction Loss (복원):** "데이터를 완벽하게 복구하고 싶어!"
    
    - Encoder에게 분산($\sigma$)을 0으로 만들라고 압박합니다. 분산이 없어야(점으로 찍어야) Decoder가 복원하기 쉬우니까요. (AE처럼 되려고 함)
        
2. **KL Divergence (정규화):** "잠재 공간을 예쁘게 정리하고 싶어!"
    
    - 모든 잠재 변수의 분포가 표준정규분포 $\mathcal{N}(0, I)$를 따르도록 강제합니다.
        
    - 데이터들을 원점 주변으로 모으고(평균 $\to$ 0), 적당히 퍼뜨려서(분산 $\to$ 1) 빈 공간을 없앱니다.
        

#### **C. 결과: 매끄러운 잠재 공간 (Smooth Latent Manifold)**

이 줄다리기 덕분에 VAE의 잠재 공간은 다음과 같은 특징을 가집니다.

- **밀집성 (Dense):** KL 항 때문에 데이터들이 원점 근처에 옹기종기 모입니다.
    
- **연속성 (Continuous):** 비슷한 이미지들은 서로 겹치는 분포를 가집니다. 따라서 $z_1$(고양이)과 $z_2$(개) 사이의 중간 지점을 찍으면, "개냥이" 같은 자연스러운 중간 이미지가 생성됩니다.
    

결론적으로 VAE는:

데이터를 억지로 외우는 게 아니라(Overfitting), 데이터가 생성되는 **근본적인 확률 규칙(Manifold)**을 학습하여, 존재하지 않았던 새로운 데이터를 '그럴싸하게' 상상해 낼 수 있는 능력을 갖추게 됩니다.