



## MLE (With ELBO)
![[Pasted image 20251213215457.png]]

![[Pasted image 20251213220152.png]]

### 1. Reconstruction Error

encoder로부터 z를 sampling하고, z를 decoder에 넣어서 p(x|z)를 계산함으로써 계산할 수 있다. 

### Regularization Error
q와 p가 둘 다 정규분포이기 때문에, 정규분포 두 개 사이의 KL divergence는 쉽게 계산이 가능하다. 하지만 **세 번째 항은 우리가 p(z|x)를 알 수가 없기 때문에 계산하는 것이 불가능**








## 수학적 의의

#### **A. "점"이 아니라 "영역"으로 매핑한다**

- **일반 AE:** 입력 이미지를 잠재 공간의 좌표 $(2.5, -1.3)$ 같은 **점**으로 보냅니다.
    
- **VAE:** 입력 이미지를 $(2.5, -1.3)$을 중심으로 하고 반지름(분산)이 있는 **구름(분포)**으로 보냅니다.
    
    - Encoder는 "이 이미지는 대략 이 근처 쯤에 있어"라고 **불확실성(Uncertainty)**을 포함해서 알려주는 것입니다.
        

#### **B. 두 가지 힘의 줄다리기 (Trade-off)**

Loss Function의 두 항은 서로 상충되는 목적을 가집니다.

1. **Reconstruction Loss (복원):** "데이터를 완벽하게 복구하고 싶어!"
    
    - Encoder에게 분산($\sigma$)을 0으로 만들라고 압박합니다. 분산이 없어야(점으로 찍어야) Decoder가 복원하기 쉬우니까요. (AE처럼 되려고 함)
        
2. **KL Divergence (정규화):** "잠재 공간을 예쁘게 정리하고 싶어!"
    
    - 모든 잠재 변수의 분포가 표준정규분포 $\mathcal{N}(0, I)$를 따르도록 강제합니다.
        
    - 데이터들을 원점 주변으로 모으고(평균 $\to$ 0), 적당히 퍼뜨려서(분산 $\to$ 1) 빈 공간을 없앱니다.
        

#### **C. 결과: 매끄러운 잠재 공간 (Smooth Latent Manifold)**

이 줄다리기 덕분에 VAE의 잠재 공간은 다음과 같은 특징을 가집니다.

- **밀집성 (Dense):** KL 항 때문에 데이터들이 원점 근처에 옹기종기 모입니다.
    
- **연속성 (Continuous):** 비슷한 이미지들은 서로 겹치는 분포를 가집니다. 따라서 $z_1$(고양이)과 $z_2$(개) 사이의 중간 지점을 찍으면, "개냥이" 같은 자연스러운 중간 이미지가 생성됩니다.
    

결론적으로 VAE는:

데이터를 억지로 외우는 게 아니라(Overfitting), 데이터가 생성되는 **근본적인 확률 규칙(Manifold)**을 학습하여, 존재하지 않았던 새로운 데이터를 '그럴싸하게' 상상해 낼 수 있는 능력을 갖추게 됩니다.