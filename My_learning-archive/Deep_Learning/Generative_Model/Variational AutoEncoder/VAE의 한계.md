## VAE의 한계 ①: posterior 근사 문제 → Normalizing Flow로 이어짐

왜 p(z∣x)p(z|x)p(z∣x) 대신 q(z∣x)q(z|x)q(z∣x)를 쓰나?

- 디코더가 매우 복잡하고 비선형이라  
    p(z∣x)p(z|x)p(z∣x)를 **직접 계산/표현하기가 거의 불가능**.
    
- 그래서 **근사 분포 qqq** 를 둔 것인데,
    
    - 이게 근사이기 때문에 정확한 posterior와 차이가 남.
        

그 해결 방향 중 하나:

- 디코더를 **완전히 invertible** 하게 설계하면,
    
- x↔zx \leftrightarrow zx↔z를 정확히 역변환할 수 있고,
    
- 더 이상 복잡한 posterior 근사 qqq가 필요 없게 된다.
    
- 이런 아이디어가 바로 **Normalizing Flow** 류 모델.
    

강의에서는:

> “이건 다음 수업에서 다룰 Normalizing Flow로 연결된다.”

라고 예고.

---

## 17. VAE의 한계 ②: 출력이 “뿌옇다(blurry)” → Diffusion으로 이어짐

슬라이드에서 예시 이미지들:

- MNIST, 얼굴 등 VAE 샘플을 보면  
    **전체적으로 흐릿한(blurry)** 느낌.
    

왜 그런가?

- 디코더는 평균적인 이미지를 만들고,
    
- 노이즈를 Gaussian으로 가정.
    
- 블러 처리된 얼굴 vs 선명한 얼굴을 생각해 보면:
    
    - 그 차이(선명한 것 – 블러된 것)도 결국 “얼굴 모양” 구조를 갖는다.
        
- 그런데 우리는 **노이즈가 “구조 없는 것”** 이라고 가정했는데,
    
    - 실제로는 오차(노이즈)가 얼굴의 구조를 포함하고 있으니  
        → 모델이 “노이즈를 완전히 decorrelate”하지 못한 것.
        
- 원인 중 하나:
    
    - prior p(z)p(z)p(z) 를 너무 단순한 N(0,I)\mathcal{N}(0,I)N(0,I) 로 잡았기 때문에,
        
    - 그로부터 복잡한 실제 데이터 분포로 가려면  
        디코더가 “너무 먼 거리”를 이동해야 함.
        
    - 이 과정에서 평균적인, 약간 뭉개진 해석을 하게 되는 경향.
        

아이디어:

- **처음부터 조금 더 데이터 분포에 가까운 prior** 를 쓰면 어떨까?
    
- 예를 들어:
    
    - 간단한 분포 → 조금 복잡한 분포 → 더 복잡한 분포…  
        이런 식으로 여러 단계로 점차적으로 가까워지도록 구성.
        
- 이러한 “prior를 점점 복잡하게 만드는 과정”을  
    잘 설계한 모델 군이 바로 **Diffusion Model**.
    

강의에서는:

> 오토인코더를 “겹겹이 씨앗(prior)”로 사용하는 방식 →  
> 그 특수한 형태가 Diffusion Model이다.  
> 이것도 다음 수업에서 다룬다고 예고.