"우리가 관측한 데이터 $x$가 나올 확률($P(x)$)을 최대화하고 싶은데, 그 식을 전개해보니 **도저히 계산할 수 없는(Intractable) 진짜 분포들**이 튀어나와서, **2가지 대담한 가정**을 통해 이를 해결한다"는 것이 VAE의 스토리입니다.

그 **"알 수 없는 분포"**가 무엇이고, **"2가지 가정"**이 무엇인지 정리해 드릴게요.

---

### 1. 우리가 부딪힌 벽: "알 수 없는 분포"

우리의 목표는 $\log P(x)$ 최대화입니다. 그런데 $P(x)$를 구하려면 잠재 변수 $z$에 대해 적분을 해야 합니다.

$$P(x) = \int P(x|z)P(z) dz$$

이 적분은 불가능합니다. 그래서 식을 **ELBO(Evidence Lower BOund)**와 **KL Divergence**로 쪼갭니다.

$$\log P(x) = \underbrace{\mathbb{E}_{q}[\log P(x|z)] - D_{KL}(Q(z|x) || P(z))}_{\text{ELBO (우리가 최대화할 것)}} + \underbrace{D_{KL}(Q(z|x) || P(z|x))}_{\text{알 수 없는 오차}}$$

여기서 **"알 수 없는 분포"**가 바로 맨 뒤에 있는 **$P(z|x)$ (True Posterior)**입니다.

- "이 이미지($x$)를 만들려면 정확히 어떤 잠재 코드($z$)가 필요해?"라는 질문에 대한 진짜 정답 분포입니다.
    
- 이걸 신(God)이 아닌 이상 알 방법이 없습니다.
    

그래서 우리는 이 알 수 없는 항을 **버리고(0보다 크니까)**, 앞부분(ELBO)만이라도 최대화하자는 전략을 씁니다. 이때 **두 가지 가정**이 들어갑니다.

---

### 2. 해결을 위한 2가지 가정

계산 가능한 식으로 만들기 위해 우리는 **"분포의 모양"**에 대해 가정을 합니다.

#### 가정 1: "진짜 분포($P(z|x)$)를 흉내 내는 가짜 분포($Q(z|x)$)를 쓰자." (Variational Inference)

- **문제:** 진짜 후전 분포 $P(z|x)$를 모르니까 적분을 못 함.
    
- **가정:** "우리가 아는 만만한 함수(뉴럴 네트워크) $Q_{\phi}(z|x)$가 $P(z|x)$를 근사할 수 있다고 치자."
    
    - 이 $Q$가 바로 **Encoder**입니다.
        
    - Encoder가 뱉어내는 값으로 잠재 변수 $z$의 분포를 정의합니다.
        

#### 가정 2: "모든 분포는 가우시안(정규분포)이다." (Gaussian Assumption)

- **문제:** $Q$를 도입했어도, 수식이 복잡하면(미분 불가능하면) 컴퓨터가 계산을 못 함. 특히 KL Divergence 항을 계산하려면 적분이 또 나옴.
    
- **가정:** "세상 모든 건 **정규분포(Gaussian)**라고 치자."
    
    1. **Prior $P(z)$:** 잠재 공간의 분포는 평균 0, 분산 1인 표준 정규분포($\mathcal{N}(0, I)$)다.
        
    2. **Posterior $Q(z|x)$:** Encoder가 뱉는 것도 정규분포($\mathcal{N}(\mu, \sigma^2)$)다.
        
    3. **Likelihood $P(x|z)$:** Decoder가 뱉는 것도 (MSE Loss를 쓴다면) 정규분포다.
        
- **효과:**
    
    - 두 정규분포 사이의 KL Divergence는 **적분 없이 깔끔한 수식(Closed-form)**으로 딱 떨어집니다.
        
    - $$D_{KL} = -0.5 \sum (1 + \log(\sigma^2) - \mu^2 - \sigma^2)$$
        
    - 이 가정 덕분에 **Reparameterization Trick**($z = \mu + \sigma \odot \epsilon$)을 써서 미분이 가능해집니다.
        

---

### 요약

1. **목적:** $\log P(x)$를 높이자.
    
2. **난관:** $P(z|x)$라는 **알 수 없는 분포**가 있어서 계산 불가.
    
3. **해결 (가정 2개):**
    
    - (1) **Encoder($Q$)**라는 녀석이 그 알 수 없는 분포를 대신한다고 치자.
        
    - (2) 그리고 모든 분포는 계산하기 쉬운 **가우시안**이라고 치자.
        

이 두 가지 가정 덕분에 그 복잡한 확률론 문제가 **"Reconstruction Error(복원 오차) 줄이기 + 정규분포 모양 맞추기(KL)"**라는 아주 심플한 딥러닝 손실 함수로 바뀌게 된 것입니다.

---

### **VAE의 주요 한계와 해결 방향**

#### **1. 한계 ①: Posterior 근사 문제 (The Inference Gap)**

- **VAE의 한계:** VAE는 우리가 모르는 $P(z|x)$ 대신 단순한 형태(주로 가우시안)의 $Q(z|x)$를 사용하는데, 이 근사 때문에 **정확한 $z$를 추론하지 못하는 오차(Inference Gap)**가 발생합니다.
    
- **Normalizing Flow로의 연결:** 만약 데이터의 변환 $z \to x$와 역변환 $x \to z$가 모두 **정확하게 계산 가능(가역적, Invertible)**하도록 모델을 설계한다면, $P(z|x)$를 추론하기 위한 별도의 근사 분포 $Q(z|x)$ 자체가 필요 없게 됩니다. 이 아이디어를 발전시킨 것이 바로 **Normalizing Flow** 모델입니다.

#### **2. 한계 ②: 출력이 "뿌옇다 (Blurry)" 문제**

| **문제**                   | **원인**                                                                            | **해결 아이디어 → 모델**                                                 |
| ------------------------ | --------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **샘플 이미지의 품질 저하 (흐릿함).** | 디코더가 **평균적인 이미지**를 생성하는 경향이 있으며, 노이즈를 $\mathcal{N}(0, I)$라는 구조 없는 가우시안으로 가정했기 때문. | **단순한 $P(z)$ Prior를 여러 단계로 점진적으로 복잡하게** 만들어 실제 데이터 분포에 가까워지도록 함. |
| **핵심**                   | $P(z)$ (Prior)가 너무 단순하여, 복잡한 데이터 분포까지 한 번에 도약하기 어려움. $\rightarrow$ 평균적인 해석 초래.    | $\Downarrow$                                                     |
| **다음 모델**                |                                                                                   | **Diffusion Model** (다음 단계 모델)                                   |

VAE는 생성 모델의 중요한 기반을 다졌으나, 위 두 가지 한계를 극복하기 위해 **Normalizing Flow**와 **Diffusion Model**이라는 새로운 방법론이 발전하게 됩니다.