
최근 **DeepSeek-V2** 모델에서 제안된 획기적인 어텐션 구조로, 
기존의 메모리 효율화 방식인 **MQA/GQA와는 접근 방식 자체가 다릅니다.**

**"K, V의 개수를 줄이는 것(GQA)" 이 아니라, "K, V 자체를 압축(Compression)하는 것"이 핵심입니다. 이를 기술적으로 Low-Rank Key-Value Joint Compression**이라고 부릅니다.

---

### 1. 핵심 아이디어: "책을 공유하지 말고, 내용을 요약해서 저장하자"

기존 방식과 MLA의 차이를 도서관 비유로 설명해 보겠습니다.

- **MHA (기본):** 사서 8명이 각자 **다른 백과사전(KV)**을 가지고 있음. (메모리 8개)
    
- **GQA (Llama-2/3):** 사서 8명이 **백과사전 1권을 돌려봄**. (메모리 1개, 하지만 정보 다양성 부족 가능성)
    
- **MLA (DeepSeek-V2):** 사서 8명이 각자 다른 정보를 보고 싶어 함. 백과사전을 통째로 저장하는 대신, **핵심 내용만 적힌 '요약 쪽지(Latent Vector)'**만 저장함. 필요할 때 이 쪽지를 보고 내용을 복원해서 씀. (메모리 작음 + 정보 다양성 유지)
    

---

### 2. 작동 원리 (Step-by-Step)

MLA는 **압축(Down-projection)**과 **복원(Up-projection)** 과정을 거치며, 여기에 **위치 정보(RoPE)**를 따로 처리하는 디테일이 숨어 있습니다.

#### ① KV 압축 (Low-Rank Joint Compression)

입력 벡터 $h$가 들어오면, 바로 거대한 $K, V$ 행렬을 만들지 않습니다. 대신 **훨씬 작은 차원($d_c$)의 잠재 벡터(Latent Vector) $c_{KV}$**로 압축합니다.

$$c_{KV} = h \times W_{down} \quad (\text{차원: 매우 작음})$$

- 이 **$c_{KV}$**만 KV Cache에 저장합니다. 이것이 메모리 절약의 핵심입니다. 
    ---> 1 x d_c

#### ② 어텐션 계산 시의 '흡수(Absorption)' 트릭 **(중요)**

"압축했으면 어텐션 할 때 다시 풀어서(Up-projection) 써야 하니까 연산량이 늘어나는 거 아닌가?"라는 의문이 들 수 있습니다.

하지만 MLA는 수학적 트릭을 써서 풀어내는 과정을 Query($Q$) 쪽으로 넘겨버립니다.

- **원래 식:** $\text{Score} = Q \times K^T = Q \times (c_{KV} \times W_{up})^T$
    
- **변형 식:** $\text{Score} = (Q \times W_{up}^T) \times c_{KV}^T$
     

즉, 추론 시에는 Query 벡터 $Q$를 미리 변환시켜 놓으면, 압축된 $c_{KV}$와 직접 연산할 수 있습니다.

$\rightarrow$ 결과: 메모리는 적게 쓰면서(압축), 연산 속도 저하는 거의 없습니다(행렬 결합).

#### ③ MLA의 실제 구현: Decoupled RoPE

DeepSeek-V2의 MLA는 $Q$와 $K$를 만들 때 아예 두 개의 독립적인 벡터를 생성해서 옆으로 붙입니다.
##### (1) Key의 구조 (저장되는 값)

Key를 만들 때 처음부터 두 갈래로 나눕니다.

1. **Content Key ($c_{KV}$):**
    
    - 내용을 담고 있으며, **압축**합니다.
        
    - RoPE를 적용하지 **않습니다.**
        
2. **RoPE Key ($k_{rope}$):**
    
    - 위치 정보를 담기 위한 전용 벡터를 따로 만듭니다. (압축하지 않음)
        
    - 여기에만 **RoPE를 적용**합니다.
        

> **캐시에 저장되는 것:** `[압축된 c_KV]` 와 `[RoPE 먹인 k_rope]` 두 개를 따로 저장합니다.

##### (2) Query의 구조 (추론 시)

Query도 똑같이 두 갈래로 맞춰줍니다.

1. **Content Query ($q_{content}$):** RoPE 없이 내용만 담음.
    
2. **RoPE Query ($q_{rope}$):** RoPE를 적용한 위치 전용 쿼리.
    

##### (3) 어텐션 스코어 계산 (합체)

이제 어텐션 스코어는 두 부분의 합으로 계산됩니다.

$$\text{Score} = \underbrace{(Q_{content} W_{up}^T) \times c_{KV}^T}_{\text{압축된 내용 매칭 (RoPE 없음)}} + \underbrace{Q_{rope} \times K_{rope}^T}_{\text{위치 매칭 (RoPE 있음)}}$$

---

### 3. 왜 성능이 더 좋은가? (가설: Regularization Effect)

사용자님께서 언급하신 "기본 Attention보다 성능이 좋을 수 있다"는 관찰은 매우 흥미로운 포인트이며, 실제로 DeepSeek 팀도 이를 강점으로 내세웁니다.

#### 이유 1: 노이즈 제거 (Denoising)

Low-Rank Compression(저랭크 압축)은 PCA(주성분 분석)와 원리가 비슷합니다. 데이터를 낮은 차원으로 억지로 구겨 넣으면, 중요하지 않은 자잘한 정보(Noise)는 사라지고, 데이터의 가장 핵심적인 특징(Principal Component)만 남게 됩니다.

이 과정이 모델에게 "쓸데없는 건 외우지 마"라고 강제하는 정규화(Regularization) 효과를 줍니다.

#### 이유 2: 헤드 간의 유연한 정보 공유

GQA는 강제로 K, V를 공유시켰지만, MLA는 $W_{up}$ (Up-projection) 행렬을 통해 **하나의 압축된 잠재 벡터($c_{KV}$)에서 각 헤드가 필요한 정보를 스스로 복원**해냅니다.

- 헤드 1: "난 $c_{KV}$에서 문법 정보만 뽑아갈래."
    
- 헤드 2: "난 $c_{KV}$에서 의미 정보만 뽑아갈래."
    
    이처럼 동적이고 유연한 정보 생성이 가능해져 표현력(Expressiveness)이 MHA(기본)보다 오히려 나아질 수 있습니다.
    

---

### 요약 비교표

|**구분**|**MHA (Standard)**|**GQA (Llama 3)**|**MLA (DeepSeek-V2)**|
|---|---|---|---|
|**방식**|K, V 모두 저장|K, V 개수 줄여서 공유|**K, V를 압축해서 저장 ($c_{KV}$)**|
|**KV Cache 크기**|매우 큼 (100%)|작음 (1/8 ~ 1/4)|**매우 작음 (GQA 수준)**|
|**위치 정보**|벡터 전체에 적용|벡터 전체에 적용|**따로 떼어서 보존 (Decoupled RoPE)**|
|**성능**|최상|약간 하락 가능성|**최상 (MHA 이상일 수도)**|
|**특징**|메모리 먹는 하마|메모리 효율적|**메모리 효율 + 성능 잡음**|

결론적으로 MLA는:

"KV Cache가 너무 커서 문제인데, 무작정 개수를 줄이니(GQA) 똑똑함이 줄어드는 것 같아. 그러니 정보를 압축(Low-Rank)해서 저장하고, 위치 정보(RoPE)만 따로 관리하자. 그랬더니 불필요한 정보가 걸러져서(Regularization) 오히려 더 똑똑해졌다!"라는 결론에 도달한 아키텍처입니다.