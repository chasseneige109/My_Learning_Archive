
최근 **DeepSeek-V2** 모델에서 제안된 획기적인 어텐션 구조로, 
기존의 메모리 효율화 방식인 **MQA/GQA와는 접근 방식 자체가 다릅니다.**

**"K, V의 개수를 줄이는 것(GQA)" 이 아니라, "K, V 자체를 압축(Compression)하는 것"이 핵심입니다. 이를 기술적으로 Low-Rank Key-Value Joint Compression**이라고 부릅니다.

---

### 1. 핵심 아이디어: "책을 공유하지 말고, 내용을 요약해서 저장하자"

기존 방식과 MLA의 차이를 도서관 비유로 설명해 보겠습니다.

- **MHA (기본):** 사서 8명이 각자 **다른 백과사전(KV)**을 가지고 있음. (메모리 8개)
    
- **GQA (Llama-2/3):** 사서 8명이 **백과사전 1권을 돌려봄**. (메모리 1개, 하지만 정보 다양성 부족 가능성)
    
- **MLA (DeepSeek-V2):** 사서 8명이 각자 다른 정보를 보고 싶어 함. 백과사전을 통째로 저장하는 대신, **핵심 내용만 적힌 '요약 쪽지(Latent Vector)'**만 저장함. 필요할 때 이 쪽지를 보고 내용을 복원해서 씀. (메모리 작음 + 정보 다양성 유지)
    

---

### 2. 작동 원리 (Step-by-Step)

MLA는 **압축(Down-projection)**과 **복원(Up-projection)** 과정을 거치며, 여기에 **위치 정보(RoPE)**를 따로 처리하는 디테일이 숨어 있습니다.

#### ① KV 압축 (Low-Rank Joint Compression)

입력 벡터 $h$가 들어오면, 바로 거대한 $K, V$ 행렬을 만들지 않습니다. 대신 **훨씬 작은 차원($d_c$)의 잠재 벡터(Latent Vector) $c_{KV}$**로 압축합니다.

$$c_{KV} = h \times W_{down} \quad (\text{차원: 매우 작음})$$

- 이 **$c_{KV}$**만 KV Cache에 저장합니다. 이것이 메모리 절약의 핵심입니다. 
    ---> 1 x d_c

#### ② 어텐션 계산 시의 '흡수(Absorption)' 트릭 **(중요)**

"압축했으면 어텐션 할 때 다시 풀어서(Up-projection) 써야 하니까 연산량이 늘어나는 거 아닌가?"라는 의문이 들 수 있습니다.

하지만 MLA는 수학적 트릭을 써서 풀어내는 과정을 Query($Q$) 쪽으로 넘겨버립니다.

- **원래 식:** $\text{Score} = Q \times K^T = Q \times (c_{KV} \times W_{up})^T$
    --> 
- **변형 식:** $\text{Score} = (Q \times W_{up}^T) \times c_{KV}^T$
    

즉, 추론 시에는 Query 벡터 $Q$를 미리 변환시켜 놓으면, 압축된 $c_{KV}$와 직접 연산할 수 있습니다.

$\rightarrow$ 결과: 메모리는 적게 쓰면서(압축), 연산 속도 저하는 거의 없습니다(행렬 결합).

#### ③ Decoupled RoPE (위치 정보는 압축 금지)

압축을 하면 벡터의 차원이 줄어들고 정보가 섞이게 됩니다. 그런데 **RoPE(회전 위치 임베딩)**는 각 차원의 순서와 회전 정보가 생명입니다. 압축해버리면 위치 정보가 망가집니다.

그래서 MLA는 **K를 두 부분으로 쪼갭니다.**

1. **Content Part:** 내용 정보 $\rightarrow$ **압축함 ($c_{KV}$)**
    
2. **RoPE Part:** 위치 정보 $\rightarrow$ **압축 안 함 ($k_{rope}$)**
    

캐시에는 `[압축된 내용 벡터, 압축 안 된 위치 벡터]` 이 두 가지를 저장합니다.

---

### 3. 왜 성능이 더 좋은가? (가설: Regularization Effect)

사용자님께서 언급하신 "기본 Attention보다 성능이 좋을 수 있다"는 관찰은 매우 흥미로운 포인트이며, 실제로 DeepSeek 팀도 이를 강점으로 내세웁니다.

#### 이유 1: 노이즈 제거 (Denoising)

Low-Rank Compression(저랭크 압축)은 PCA(주성분 분석)와 원리가 비슷합니다. 데이터를 낮은 차원으로 억지로 구겨 넣으면, 중요하지 않은 자잘한 정보(Noise)는 사라지고, 데이터의 가장 핵심적인 특징(Principal Component)만 남게 됩니다.

이 과정이 모델에게 "쓸데없는 건 외우지 마"라고 강제하는 정규화(Regularization) 효과를 줍니다.

#### 이유 2: 헤드 간의 유연한 정보 공유

GQA는 강제로 K, V를 공유시켰지만, MLA는 $W_{up}$ (Up-projection) 행렬을 통해 **하나의 압축된 잠재 벡터($c_{KV}$)에서 각 헤드가 필요한 정보를 스스로 복원**해냅니다.

- 헤드 1: "난 $c_{KV}$에서 문법 정보만 뽑아갈래."
    
- 헤드 2: "난 $c_{KV}$에서 의미 정보만 뽑아갈래."
    
    이처럼 동적이고 유연한 정보 생성이 가능해져 표현력(Expressiveness)이 MHA(기본)보다 오히려 나아질 수 있습니다.
    

---

### 요약 비교표

|**구분**|**MHA (Standard)**|**GQA (Llama 3)**|**MLA (DeepSeek-V2)**|
|---|---|---|---|
|**방식**|K, V 모두 저장|K, V 개수 줄여서 공유|**K, V를 압축해서 저장 ($c_{KV}$)**|
|**KV Cache 크기**|매우 큼 (100%)|작음 (1/8 ~ 1/4)|**매우 작음 (GQA 수준)**|
|**위치 정보**|벡터 전체에 적용|벡터 전체에 적용|**따로 떼어서 보존 (Decoupled RoPE)**|
|**성능**|최상|약간 하락 가능성|**최상 (MHA 이상일 수도)**|
|**특징**|메모리 먹는 하마|메모리 효율적|**메모리 효율 + 성능 잡음**|

결론적으로 MLA는:

"KV Cache가 너무 커서 문제인데, 무작정 개수를 줄이니(GQA) 똑똑함이 줄어드는 것 같아. 그러니 정보를 압축(Low-Rank)해서 저장하고, 위치 정보(RoPE)만 따로 관리하자. 그랬더니 불필요한 정보가 걸러져서(Regularization) 오히려 더 똑똑해졌다!"라는 결론에 도달한 아키텍처입니다.