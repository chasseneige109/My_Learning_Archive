**Jensen-Shannon Divergence (JSD)**의 정의가 그렇게 생긴 이유는 **KL Divergence가 가진 치명적인 두 가지 수학적 결함(비대칭성, 무한대 발산)**을 해결하기 위해서입니다.

JSD의 정의를 먼저 보고, 왜 굳이 **"두 분포의 평균($M$)"**을 도입했는지 그 설계 의도를 뜯어보겠습니다.

---

### **1. JSD의 정의 (Recap)**

$$\text{JSD}(P \| Q) = \frac{1}{2} D_{KL}(P \| M) + \frac{1}{2} D_{KL}(Q \| M)$$

$$\text{where } M = \frac{1}{2}(P + Q) \quad (\text{평균 분포, Mixture Distribution})$$

즉, $P$와 $Q$ 사이의 거리를 직접 재는 게 아니라, **둘을 섞은 평균 분포 $M$을 가운데 두고, $P$와 $Q$가 이 $M$으로부터 얼마나 떨어져 있는지**를 재서 더하는 방식입니다.

---

### **2. 왜 이렇게 정의했을까? (설계 의도)**

#### **이유 ①: 대칭성 (Symmetry) 확보**

KL Divergence의 가장 큰 문제는 **거리(Distance)**로 쓰기에 부적합하다는 점입니다.

$$D_{KL}(P \| Q) \neq D_{KL}(Q \| P)$$

내가 너에게 가는 거리와 네가 나에게 오는 거리가 다릅니다.

하지만 JSD는 정의상 $P$와 $Q$의 위치를 바꿔도 식의 결과가 같습니다.

$$\text{JSD}(P \| Q) = \text{JSD}(Q \| P)$$

이로써 진정한 의미의 **"거리(Metric)"**에 가까워집니다. (실제로 $\sqrt{\text{JSD}}$는 거리 공간의 조건을 만족합니다.)

#### **이유 ②: 무한대 발산 방지 (Finite Value) ★ 핵심**

딥러닝(특히 GAN)에서 KL의 가장 치명적인 단점은 **두 분포가 겹치지 않을 때(Disjoint)** 발생합니다.

- KL의 문제:
    
    $$D_{KL}(P \| Q) = \sum P(x) \log \frac{P(x)}{Q(x)}$$
    
    만약 $P(x) > 0$인 곳에서 $Q(x) = 0$이라면(즉, 서로 안 겹치면), 분모가 0이 되어 **값이 무한대($\infty$)**로 폭발합니다. 학습이 터져버리죠.
    
- JSD의 해결책 ($M$의 도입):
    
    JSD는 $Q$가 아니라 $M = \frac{P+Q}{2}$를 분모로 씁니다.
    
    $$M(x) = \frac{P(x) + Q(x)}{2}$$
    
    - $P(x)$가 0이 아니면 $M(x)$도 절대 0이 아닙니다.
        
    - $Q(x)$가 0이 아니면 $M(x)$도 절대 0이 아닙니다.
        
    - 즉, **두 분포가 아예 안 겹쳐도 분모가 0이 되는 일이 절대 발생하지 않습니다.**
        

#### **이유 ③: 값의 범위 고정 (Bounded)**

KL Divergence는 $0$에서 $\infty$까지 범위를 가집니다. 값이 얼마나 커질지 모릅니다.

반면 JSD는 항상 **유계(Bounded)**입니다.

$$0 \le \text{JSD}(P \| Q) \le \ln 2 \quad (\text{약 } 0.693)$$

(로그의 밑을 2로 하면 $0 \le \text{JSD} \le 1$)

이는 손실 함수(Loss Function)로 사용할 때 값이 안정적이라는 큰 장점을 줍니다.

---

### **3. 직관적 해석: "중간 지점에서 만나자"**

JSD의 정의를 직관적인 비유로 설명하면 다음과 같습니다.

- **KL Divergence:** "서울($P$)에서 부산($Q$)까지 가는 편도 비용은 얼마인가?" (부산에서 서울로 오는 비용과는 다를 수 있음. 길이 없으면 비용 무한대.)
    
- **JSD:** "서울($P$)과 부산($Q$)의 **정중앙인 대전($M$)**을 잡자. 서울 $\to$ 대전 비용과, 부산 $\to$ 대전 비용을 합치자."
    
    - 서울이나 부산 어디서든 대전으로는 갈 수 있으므로(평균이므로), 길이 끊길(발산할) 염려가 없습니다.
        

---

### **요약**

JSD가 $\frac{1}{2}KL(P|M) + \frac{1}{2}KL(Q|M)$으로 정의된 이유는,

**"두 분포가 서로 겹치지 않아도(Support가 달라도) 무한대로 발산하지 않고, 순서를 바꿔도 값이 같은 안정적인 거리 척도"**를 만들기 위해서입니다.

이 특성 때문에 초기 **GAN(Generative Adversarial Networks)**의 손실 함수 증명 과정에서 JSD가 핵심적으로 등장합니다. (하지만 JSD도 두 분포가 안 겹칠 때 기울기(Gradient)가 사라지는 문제인 **Vanishing Gradient** 문제가 있어, 나중에는 **Wasserstein Distance (WGAN)**가 등장하게 됩니다.)