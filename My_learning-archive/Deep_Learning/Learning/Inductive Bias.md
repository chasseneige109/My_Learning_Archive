**Inductive Bias (귀납적 편향)**는 기계 학습(Machine Learning)과 딥러닝에서 모델이 **훈련 데이터에 없는 새로운 데이터를 일반화(Generalize)할 때 사용하는 기본적인 가정이나 선입견**을 의미합니다.

쉽게 말해, "모델이 정답을 찾기 위해 **미리 가지고 있는 힌트**" 또는 "세계를 바라보는 **고정된 관점**"입니다.

---

### 1. Inductive Bias의 정의

- **귀납(Induction):** 개별적인 사례(훈련 데이터)를 가지고 일반적인 규칙(모델)을 만들어내는 과정.
    
- **편향(Bias):** 학습 과정에서 수많은 가설(Hypothesis) 중 특정 가설을 다른 가설보다 선호하게 만드는 경향.
    

Inductive Bias는 **훈련 데이터만으로는 답을 정할 수 없을 때, 모델이 임의로 결정을 내리는 데 도움을 주는 원칙**입니다.

### 2. 왜 필요한가? (No Free Lunch Theorem)

Inductive Bias가 없다면, 모델은 훈련 데이터에 있는 것만 기억할 뿐, **처음 보는 데이터(테스트 데이터)**에 대해 합리적인 예측을 할 수 없습니다.

**No Free Lunch Theorem**에 따르면, 모든 문제에서 최고의 성능을 내는 단일 모델은 존재하지 않습니다. 즉, **특정 문제에 잘 맞도록 모델에 적절한 편향(Bias)을 심어줘야** 해당 문제에서 좋은 성능을 발휘할 수 있습니다.

### 3. CNN vs. ViT의 비교 (가장 대표적인 예시)

사용자님께서 ViT에 대해 질문하셨으므로, CNN과 ViT를 통해 Inductive Bias의 차이를 극명하게 볼 수 있습니다.

|**구분**|**CNN (Convolutional Neural Network)**|**ViT (Vision Transformer)**|
|---|---|---|
|**Inductive Bias**|**강함 (Strong)**|**약함 (Weak)**|
|**핵심 가정**|**1. 지역성(Locality):** 근처 픽셀끼리만 강하게 관련 있다.|**가정 없음:** 모든 패치(토큰)는 서로 독립적이며, 그 관계를 데이터로 배워야 한다.|
|**구현 요소**|**Convolution Filter:** 좁은 영역(3x3)만 보게 강제|**Self-Attention:** 첫 레이어부터 이미지 전체를 보게 허용|
|**데이터 효율**|적은 데이터로도 빠르게 학습 (힌트가 내장됨)|**대규모 데이터셋 필수** (힌트가 없어서 스스로 배워야 함)|

### 4. 다양한 모델에서의 Inductive Bias

|**모델**|**Inductive Bias**|**설명**|
|---|---|---|
|**RNN/LSTM**|**시간적 인과 관계 (Temporal Locality)**|순서대로 들어오는 시퀀스 데이터에서 가까운 시점의 정보가 더 중요하다고 가정함.|
|**Transformer**|**셋 인베리언스 (Set Invariance)**|입력 시퀀스의 순서를 무시하고 병렬 처리하며, 순서 정보는 따로 Positional Encoding으로 주입해야 함.|
|**Graph Neural Network (GNN)**|**연결성 (Connectivity)**|인접한 노드(Node)끼리 정보가 전달되어야 의미가 있다고 가정함.|

### 결론

Inductive Bias는 모델 설계의 핵심입니다.

- **강한 Bias (CNN):** 데이터가 적을 때 유리하지만, 데이터가 많아지면 모델의 유연성을 떨어뜨립니다.
    
- **약한 Bias (ViT/Transformer):** 데이터가 많을 때, 모델이 **데이터 자체에서 최적의 규칙**을 발견하도록 허용하여 더 높은 잠재력을 발휘합니다.