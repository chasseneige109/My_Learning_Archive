## 1. 시작점: 자연어 시퀀스의 근본적인 문제

자연어 문장은 **길이가 제각각**이에요.

- “Hi” → 길이 2
    
- “I love deep learning” → 길이 4
    
- “이 모델은 패딩에 대해 물어보는 사용자의 질문에 상당히 길게 답변한다” → 훨씬 김
    

그런데 GPU에서 연산을 할 때는:

- 보통 `batch_size x seq_len x hidden_dim` 같은 **직사각형 텐서**를 한 번에 처리.
    
- 텐서의 두 번째 차원(시퀀스 길이)이 **모든 샘플에서 같아야** 함.
    

> 즉, **수학적으로도, 구현적으로도 “변수 길이 텐서”를 한 번에 처리하기가 까다롭기 때문에**  
> 길이를 강제로 맞추는 장치가 필요 → 그게 `padding token`.

---

## 2. “그럼 그냥 한 문장씩 처리하면 되지 않나요?”

이론적으로는 가능해요. 하지만:

1. **GPU 병렬처리 효율이 박살납니다**
    
    - 딥러닝은 `batch` 단위로 행렬 곱을 돌리면서 GPU를 꽉 채워야 빠름.
        
    - 문장 하나씩 넣으면 연산량이 너무 작아서 GPU가 노는 시간이 많아짐.
        
2. **통계적으로도 배치 학습이 유리**
    
    - Batch Normalization이나 LayerNorm, gradient estimation 등
        
    - 여러 샘플의 손실을 평균내며 미니배치 단위로 학습하는 것이 안정적임.
        

그래서 현실적인 선택은:

> “**여러 문장을 한 텐서에 넣어야 한다 → 길이를 맞춰야 한다**”

그리고 길이를 맞추기 위한 가장 단순·일반적인 해결책:

> **짧은 문장의 뒤를 ‘의미 없는 토큰’으로 채운다 → padding token**