
### 1. 기존 방식의 한계 (Word vs. Character)

서브워드의 필요성을 이해하려면 양극단의 단점을 먼저 봐야 합니다.

|**구분**|**방식**|**문제점**|
|---|---|---|
|**단어 단위**|띄어쓰기 기준 분리<br><br>  <br><br>(예: `apple`, `apples`)|**1. OOV (Out-Of-Vocabulary) 문제:** 학습 때 본 적 없는 단어(희귀 단어, 오타)가 나오면 `[UNK]`(Unknown) 처리되어 정보를 잃음.<br><br>  <br><br>**2. 단어 집합이 너무 큼:** `work`, `works`, `worked`를 모두 다른 단어로 저장해야 해서 메모리 낭비가 심함.|
|**문자 단위**|글자 하나하나 분리<br><br>  <br><br>(예: `a`, `p`, `p`, `l`, `e`)|**1. 의미 파악 어려움:** 'a'라는 글자 자체에는 의미가 없음.<br><br>  <br><br>**2. 시퀀스가 너무 길어짐:** 문장이 너무 길어져서 모델이 문맥을 학습하기 어려움.|

---

### 2. 서브워드(Subword)의 해결책: "의미 있는 조각으로 쪼개자"

서브워드 방식(BPE, WordPiece 등)은 **"빈도수가 높은(자주 쓰이는) 문자 조합은 하나의 토큰으로 묶고, 희귀한 단어는 쪼개자"**는 전략을 취합니다.

#### ① 미등록 단어(OOV)와 희귀 단어 처리

단어 단위 토크나이저가 'chatbot'이라는 단어를 학습하지 않았다면, 이를 모르는 단어(`[UNK]`)로 취급합니다. 하지만 서브워드 방식은 이를 아는 단어들의 조합으로 분해합니다.

- **예시:** `chatbot`이라는 단어가 사전에 없을 때
    
    - **분해:** `chat` (자주 등장) + `bot` (자주 등장)
        
    - **결과:** 모델은 '대화(chat)'와 '로봇(bot)'의 의미를 결합하여 `chatbot`의 의미를 유추할 수 있습니다.
        
- **효과:** 처음 보는 신조어나 복합어가 나와도, 기존에 학습된 서브워드들의 조합으로 표현이 가능하므로 **OOV 문제가 획기적으로 줄어듭니다.**