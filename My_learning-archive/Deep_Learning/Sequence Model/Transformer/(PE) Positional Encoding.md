# ✅ 절대 위치 인코딩 (Absolute Positional Encoding)

각 위치에 대해 **“이 위치는 0번, 1번, 2번…”**처럼 절대적인 번호를 쓰는 방식.

# ✅ Sinusoidal Positional Encoding (Transformer 원 논문 방식)

일부러 p를 절댓값 p = 1이 되도록 설정함. 
위치는 원래 반지름 r과 각도 (theta)로 표현되지만, 반지름 = 1로 통제하여
회전 각도로만 표현되도록 강제함.

- Distance == Rotation이 됨.
- p1^T * p2 = cos (theta1 - theta2)    --> 내적이 두 각도의 차이만으로 표현됨.

장점:

- **문장 길이가 길어져도** 계속 계산해서 쓸 수 있음
- 파라미터를 따로 안 배워도 됨 (메모리 절약)

단점:

- 너무 수학적으로 설계된 느낌이라,  
**학습된 embedding처럼 유연하진 않음**

#  ✅ Learned Positional Embedding (학습되는 위치 임베딩)

요즘 많이 쓰는 방식:

- 단어 임베딩이랑 똑같이 생각하면 됨

장점:

- 모델이 **직접 최적의 위치 표현**을 배우므로 더 유연  
    단점:
    
- 훈련한 최대 길이보다 **더 긴 문장**에서 일반화가 약할 수 있음



# ✅상대 위치 인코딩 (Relative Positional Encoding)

위의 것들은 다 **“이 단어는 문장에서 n번째입니다”** 같은 **절대 위치** 정보.

하지만, 언어에서는 실제로:
- “앞/뒤로 얼마나 떨어져 있냐”가 더 중요할 때가 많죠.

> “위치 자체 말고, **단어들 사이의 거리**를 인코딩하자”

장점:

- “가까운 단어에 더 집중” 같은 패턴을 **더 자연스럽게 학습**
    
- 길이가 달라져도 “거리 개념”은 유지됨

# (2) RoPE (Rotary Positional Embedding)


기존 positional encoding들에는 문제가 있었음.

### ❌ Absolute positional embedding

- 위치를 더함: `x = e + p`
    
- 문제:
    - 거리 개념이 indirect
    - 훈련 길이보다 긴 문장에 약함
    - 상대 위치 정보가 깔끔하지 않음

### ❌ Sinusoidal (absolute 버전)

- 거리 구조는 있지만 여전히 “더해서” 들어감 attention 단계에서 위치가 **직접적으로** 작동하진 않음
- x = e + p


# 2️⃣ 핵심 아이디어: “위치 = 각도”

- Q, K를 **2D 회전변환**으로 위치에 따라 회전
    
- 위치 t에 대해 `R(θ_t)`라는 회전행렬을 곱하는 느낌
    
- 장점:
    
    - 학습 후 더 긴 길이로도 extrapolation 가능  
        → θ 스케일 조정으로 길이 연장