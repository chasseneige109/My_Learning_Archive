# ✅ 절대 위치 인코딩 (Absolute Positional Encoding)

각 위치에 대해 **“이 위치는 0번, 1번, 2번…”**처럼 절대적인 번호를 쓰는 방식.

# ✅ Sinusoidal Positional Encoding (Transformer 원 논문 방식)

일부러 p를 절댓값 p = 1이 되도록 설정함. 
위치는 원래 반지름 r과 각도 (theta)로 표현되지만, 반지름 = 1로 통제하여
회전 각도로만 표현되도록 강제함.

- Distance == Rotation이 됨.
- cos (theta1 - theta2)

논문에서 썼던 방식은 **사인/코사인**을 이용한 수식이에요:

- 각 위치마다 **고정된 패턴의 벡터**가 생김
- 학습으로 바뀌지 않는 **deterministic(고정)** 인코딩
- 위치 차이가 수학적으로 잘 반영되도록 설계됨

장점:

- **문장 길이가 길어져도** 계속 계산해서 쓸 수 있음
- 파라미터를 따로 안 배워도 됨 (메모리 절약)

단점:

- 너무 수학적으로 설계된 느낌이라,  
**학습된 embedding처럼 유연하진 않음**

#  Learned Positional Embedding (학습되는 위치 임베딩)

요즘 많이 쓰는 방식:

- 단어 임베딩이랑 똑같이 생각하면 됨

장점:

- 모델이 **직접 최적의 위치 표현**을 배우므로 더 유연  
    단점:
    
- 훈련한 최대 길이보다 **더 긴 문장**에서 일반화가 약할 수 있음



# 상대 위치 인코딩 (Relative Positional Encoding)

위의 것들은 다 **“이 단어는 문장에서 n번째입니다”** 같은 **절대 위치** 정보.

하지만, 언어에서는 실제로:
- “앞/뒤로 얼마나 떨어져 있냐”가 더 중요할 때가 많죠.

> “위치 자체 말고, **단어들 사이의 거리**를 인코딩하자”

장점:

- “가까운 단어에 더 집중” 같은 패턴을 **더 자연스럽게 학습**
    
- 길이가 달라져도 “거리 개념”은 유지됨

# RoPE (Rotary Position Embedding) 한 줄 설명

요즘 LLM(GPT류)에서 많이 쓰는 방식 중 하나가 **RoPE**예요.

아이디어는:

> Q, K 벡터의 각 차원쌍을 (x, y)로 보고  
> 위치에 따라 2D 회전 변환을 적용해서  
> **내적 결과에 위치 차이가 반영되게** 한다.

이것도 결국:

- 단어의 “공간적 표현”에
    
- 위치 정보를 부드럽게 입히는 한 방식
    

(완전 수식 들어가면 길어지니 여기선 감만.)


# 결국 모델에서 어떻게 쓰이냐?


- **self-attention이 “어디를 볼지” 결정할 때**
    
- 단어 정보 + 위치 정보가 **같이** 반영됨
- e + p.
- 두 단어의 거리가 작으면 내적(attention)이 커지도록 학습되도록 반영됨.