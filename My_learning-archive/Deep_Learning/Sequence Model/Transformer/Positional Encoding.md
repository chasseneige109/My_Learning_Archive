### 3-1. 절대 위치 인코딩 (Absolute Positional Encoding)

각 위치에 대해 **“이 위치는 0번, 1번, 2번…”**처럼 절대적인 번호를 쓰는 방식.

#### (1) Sinusoidal Positional Encoding (Transformer 원 논문 방식)

논문에서 썼던 방식은 **사인/코사인**을 이용한 수식이에요:

- 각 위치마다 **고정된 패턴의 벡터**가 생김
    
- 학습으로 바뀌지 않는 **deterministic(고정)** 인코딩
    
- 위치 차이가 수학적으로 잘 반영되도록 설계됨
    

장점:

- **문장 길이가 길어져도** 계속 계산해서 쓸 수 있음
    
- 파라미터를 따로 안 배워도 됨 (메모리 절약)
    

단점:

- 너무 수학적으로 설계된 느낌이라,  
    **학습된 embedding처럼 유연하진 않음**

#### (2) Learned Positional Embedding (학습되는 위치 임베딩)

요즘 많이 쓰는 방식:

- 단어 임베딩이랑 똑같이 생각하면 됨
    
- `position_id`를 하나의 “토큰”처럼 보고  
    그에 대한 임베딩을 **학습**으로 얻음
    

예:

- 최대 길이 512라고 치면,
    
    - 0~511까지 총 512개의 위치 embedding
        
    - 학습하면서 자동으로 “어떤 위치에 어떤 벡터가 좋을지” 배움
        

장점:

- 모델이 **직접 최적의 위치 표현**을 배우므로 더 유연  
    단점:
    
- 훈련한 최대 길이보다 **더 긴 문장**에서 일반화가 약할 수 있음



### 3-2. 상대 위치 인코딩 (Relative Positional Encoding)

위의 것들은 다 **“이 단어는 문장에서 n번째입니다”** 같은 **절대 위치** 정보.

하지만, 언어에서는 실제로:

- “앞/뒤로 얼마나 떨어져 있냐”가 더 중요할 때가 많죠.

> “위치 자체 말고, **단어들 사이의 거리**를 인코딩하자”

장점:

- “가까운 단어에 더 집중” 같은 패턴을 **더 자연스럽게 학습**
    
- 길이가 달라져도 “거리 개념”은 유지됨