## Residual connection의 기본 아이디어

$$\text{Output} = X + F(X)$$
- 원래 입력 x를 **그대로 통과시키는 길(=identity path)** 을 하나 깔아 두고,
- 그 위에 F(x)로 **“수정값(delta)”** 만 더하는 구조.


- X : multihead attent들어가기전에 딱 embedding이랑 PE만 된 X임.
- $X_{\text{Input}} = \text{Embedding} + \text{PE}$

- F(X)

----> 📌 Residual connection에서는 딱 이거 더하기 하나밖에 안함.


## 왜 이런 구조가 필요한가? (깊은 네트워크의 근본 문제)


1. **표현 관점:**
    
    - 어떤 task에 대해 **얕은 네트워크**가 이미 잘 해결하는 경우,
    - 더 깊게 쌓으면 “최소한 얕은 네트워크만큼은” 해야 하는데
    - 실제로는 깊어질수록 오히려 성능이 망가지는 경우 발생 (degradation problem)
        
2. **최적화/역전파 관점:**
    
    - 깊을수록 gradient가 사라지거나(exploding/vanishing)
    - 좋은 초기값을 찾기 어렵고,
    - 학습이 매우 불안정해짐.


##  “항등함수(identity)를 쉽게 만들 수 있다”는 게 핵심

일반 레이어에서 **항등함수**를 만들려면:

y = F(x) ≈ x

- F의 모든 파라미터가 “항등 변환”을 구현하는 특정 값으로 맞춰져야 함.
- 이건 최적화 관점에서 꽤 까다로운 제약.

근데 residual에서는:  y= x + F(x)

여기서 **F(x) = 0**만 학습하면:

y = x + 0 = x

즉,

> “얘는 굳이 아무것도 안 해도 돼”  
> 라는 선택지를 **아주 쉽게** 만들 수 있음.

그래서 깊은 네트워크에서:
- 꼭 필요한 층만 “의미 있는 수정값” F(x)를 내고,
- 덜 중요한 층은 F(x) ≈ 0로 수렴해서 **거의 항등**으로 남을 수도 있음.

이게 바로:

> “얕은 네트워크의 성능을 최소한 유지하면서,  
> 깊게 쌓아도 문제 없게 만드는” 기저 메커니즘.