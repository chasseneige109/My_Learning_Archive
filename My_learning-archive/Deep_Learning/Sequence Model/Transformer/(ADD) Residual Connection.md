## 2. 왜 이런 구조가 필요한가? (깊은 네트워크의 근본 문제)

### 2.1 깊어질수록 생기는 두 가지 큰 문제

1. **표현 관점:**
    
    - 어떤 task에 대해 **얕은 네트워크**가 이미 잘 해결하는 경우,
    - 더 깊게 쌓으면 “최소한 얕은 네트워크만큼은” 해야 하는데
    - 실제로는 깊어질수록 오히려 성능이 망가지는 경우 발생 (degradation problem)
        
2. **최적화/역전파 관점:**
    
    - 깊을수록 gradient가 사라지거나(exploding/vanishing)
    - 좋은 초기값을 찾기 어렵고,
    - 학습이 매우 불안정해짐.

Residual connection은 이 둘을 동시에 건드립니다.


## 3. “항등함수(identity)를 쉽게 만들 수 있다”는 게 핵심

일반 레이어에서 **항등함수**를 만들려면:

y = F(x) ≈ x

- F의 모든 파라미터가 “항등 변환”을 구현하는 특정 값으로 맞춰져야 함.
- 이건 최적화 관점에서 꽤 까다로운 제약.

근데 residual에서는:y= x + F(x)

여기서 **F(x) = 0**만 학습하면:

y=x+0=xy = x + 0 = xy=x+0=x

즉,

> “얘는 굳이 아무것도 안 해도 돼”  
> 라는 선택지를 **아주 쉽게** 만들 수 있음.

그래서 깊은 네트워크에서:

- 꼭 필요한 층만 “의미 있는 수정값” F(x)를 내고,
    
- 덜 중요한 층은 F(x) ≈ 0로 수렴해서 **거의 항등**으로 남을 수도 있음.
    

이게 바로:

> “얕은 네트워크의 성능을 최소한 유지하면서,  
> 깊게 쌓아도 문제 없게 만드는” 기저 메커니즘.