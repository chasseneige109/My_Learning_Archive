
## one-hot 기반 embedding 단점

1. vocabulary를 **미리 고정**해야 한다
    
2. 새 단어 추가가 불가능함 (차원을 늘려야 하므로)
    

이건 one-hot의 근본적 한계.

그래서 embedding 자체가 잘 작동해도  
one-hot이라는 기반 representation은 여전히 불편함.

(이건 나중에 subword tokenization으로 해결된다.)


## 1. 인코더 (Encoder): 이해하고 요약하기 🧠

인코더는 **입력 시퀀스 전체를 읽고 의미를 압축**하는 역할을 합니다.

|**항목**|**기능**|**설명**|
|---|---|---|
|**입력**|$X_1, X_2, \dots, X_N$ (입력 문장의 단어 시퀀스)||
|**처리**|RNN 또는 LSTM이 순차적으로 작동하며, 매 시간 단계마다 $\mathbf{h}_t = f(\mathbf{h}_{t-1}, \mathbf{x}_t)$ 방식으로 문맥을 **누적**함.||
|**출력**|**문맥 벡터 (Context Vector, $\mathbf{C}$)**|마지막 은닉 상태($\mathbf{h}_N$)로, 입력 시퀀스 전체의 **핵심 의미를 압축한 고정된 크기의 벡터**입니다.|
|**역할**|**압축:** 모든 입력 정보를 단 하나의 벡터로 인코딩합니다.||

## 2. 디코더 (Decoder): 해석하고 생성하기 ✍️

디코더는 인코더가 넘겨준 문맥 벡터를 바탕으로 **새로운 출력 시퀀스를 자기회귀적(Autoregressive)으로 생성**하는 역할을 합니다.

|**항목**|**기능**|**설명**|
|---|---|---|
|**초기 입력**|**인코더의 문맥 벡터 ($\mathbf{C}$)**|디코더의 첫 번째 은닉 상태($\mathbf{h}_0$)로 사용되며, 생성의 **출발점**이 됩니다.|
|**처리**|**조건부 언어 모델 (Conditional Language Model)**로 작동함. 이전 단계에서 생성한 단어($Y_{t-1}$)를 다음 단계($t$)의 입력으로 사용하여($\mathbf{h}_t = f(\mathbf{h}_{t-1}, Y_{t-1})$) 다음 단어의 확률을 예측함.||
|**출력**|$Y_1, Y_2, \dots, Y_M$ (출력 시퀀스)|문장의 끝을 알리는 특수 토큰(EOS)이 나올 때까지 단어를 하나씩 생성합니다.|
|**역할**|**생성:** 압축된 의미를 목표 언어의 시퀀스로 풀어냅니다.||