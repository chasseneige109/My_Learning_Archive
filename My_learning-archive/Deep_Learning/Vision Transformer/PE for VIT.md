### 1. 표준 방식: 학습 가능한 1D 벡터 (Learnable 1D PE)

결론부터 말씀드리면, ViT는 **이미지의 2D 구조(행, 열)를 무시하고 그냥 1부터 N까지 번호를 매긴 뒤, 각 번호에 해당하는 벡터를 더해줍니다.**

#### 과정 (Step-by-Step)

1. **패치 나열:** $16 \times 16$ 패치로 자른 이미지가 있다면, 왼쪽 위부터 오른쪽 아래 순서대로 1번부터 196번까지 줄을 세웁니다 (Flatten).
    
2. **랜덤 초기화:** 196개의 각 위치에 대해, 차원 $D$(예: 768)를 가진 **랜덤 벡터**를 할당합니다.
    
    - $E_{pos} \in \mathbb{R}^{(196+1) \times 768}$ (1은 CLS 토큰용)
        
3. **단순 더하기:** 이 위치 벡터를 패치 임베딩 벡터에 요소별로 더합니다(Element-wise Sum).
    
    - $\text{Input} = \text{Patch Embedding} + \text{Positional Embedding}$

### 2. 의문점: "1D로 폈는데 위아래(2D) 관계를 어떻게 알아?"

사용자님의 핵심 질문입니다. 1번 패치와 15번 패치가 2D상에서 바로 위아래 관계라는 것을 어떻게 알까요?

**정답: "모델이 알아서 배웁니다."**

ViT 논문 저자들도 이게 궁금해서 실험을 했습니다.

- **1D PE:** 그냥 1~196 번호만 줌.
    
- **2D PE:** $(X, Y)$ 좌표 정보를 따로따로 인코딩해서 줌.
    
- **Relative PE:** 상대적인 거리 정보만 줌.
    

**결과:** 놀랍게도 **성능 차이가 거의 없었습니다.** 1D PE만 줘도 충분했습니다.

#### 학습된 PE의 시각화 (Cosine Similarity)

학습이 끝난 후, **각 위치의 임베딩 벡터들이 서로 얼마나 비슷한지(코사인 유사도)**를 찍어보면 놀라운 패턴이 나타납니다.

- **중심 패치:** 어떤 패치(예: 정중앙)의 PE 벡터는 **자신의 바로 위, 아래, 왼쪽, 오른쪽**에 있는 패치들의 PE 벡터와 **수학적으로 매우 유사**해져 있습니다.
    
- 즉, 우리가 명시적으로 "너네는 위아래야"라고 알려주지 않았는데도, 모델이 Loss를 줄이기 위해 학습하다 보니 **"이 위치의 벡터와 저 위치의 벡터를 비슷하게 만들어야 정보 처리가 잘 되네?"** 하고 스스로 **2D 그리드 구조를 재구성**해낸 것입니다.