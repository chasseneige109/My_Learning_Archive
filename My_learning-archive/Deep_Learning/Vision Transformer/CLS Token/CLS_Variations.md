실제로 많은 연구자들이 "왜 반장(`[CLS]`)이 한 명이어야 해? 두 명이면 더 똑똑하지 않을까?"라는 생각으로 실험을 했고, 아주 흥미로운 결과들이 나왔습니다.

`[CLS]` 토큰을 2개 (혹은 그 이상) 사용하면 벌어지는 일을 **3가지 시나리오**로 나누어 설명해 드리겠습니다.

---

### 1. 시나리오 A: 별다른 설정 없이 그냥 2개 넣었을 때 (Redundancy)

만약 `[CLS1]`, `[CLS2]`를 넣고, 똑같은 방식으로 초기화하고, 마지막에 둘 다 합쳐서(Average) 분류에 쓴다면?

- **결과:** **별 차이 없거나, 둘이 똑같아집니다.**
    
- **이유 (Symmetry Problem):** 두 토큰이 보는 정보(패치들)가 똑같고, 목표(Loss Function)도 똑같다면, 학습 과정에서 두 벡터는 거의 동일한 값을 가지게 됩니다. 마치 반장을 2명 뽑았는데 둘이 똑같은 말만 하는 상황이 됩니다. 메모리만 낭비하는 셈이죠.
    

---

### 2. 시나리오 B: 역할을 나눠줬을 때 (DeiT - Distillation Token)

이게 실제로 **DeiT (Data-efficient Image Transformers)**라는 유명한 논문에서 쓴 방법입니다. 여기서는 토큰을 2개 씁니다.

1. **`[CLS]` 토큰:** 정답 라벨(Ground Truth)을 맞히는 역할. (반장: 선생님 말씀 잘 듣기)
    
2. **`[Distill]` 토큰:** 이미 학습된 **선생님 모델(Teacher Network, 예: CNN)**의 출력을 흉내 내는 역할. (부반장: 공부 잘하는 형 따라 하기)
    

- **결과:** 두 토큰은 서로 다른 목적을 가지고 학습하므로, **서로 다른 정보**를 흡수합니다. 나중에 이 둘을 합쳐서 사용하면 **성능이 비약적으로 상승**합니다. ViT가 데이터가 적을 때 학습이 잘 안 되는 문제를 이걸로 해결했습니다.
    

---

### 3. 시나리오 C: "쓰레기통"으로 쓸 때 (ViT Registers)

최근(2023년) **"Vision Transformers Need Registers"**라는 논문에서 밝혀진 아주 재미있는 현상입니다.

- **현상:** ViT를 학습시키다 보니, 모델이 이미지의 배경 부분(하늘, 잔디 등)에 자꾸 이상하게 높은 값을 저장하는 현상(Artifact)이 발견됐습니다. 정보를 처리할 공간이 부족해서, 쓸모없는 배경 패치에다가 "임시 메모리"처럼 정보를 쑤셔 넣고 있었던 거죠.
    
- **해결:** `[CLS]` 외에 **아무 역할도 안 하는 빈 토큰 4~8개(Register Tokens)**를 더 넣어줬습니다.
    
- **결과:** 모델이 **"아, 여기다가는 잡다한 정보를 버리면 되는구나!"** 하고 이 추가 토큰들을 **정보의 쓰레기통(Sink)**이나 **임시 저장소**로 사용하기 시작했습니다.
    
    - 덕분에 진짜 `[CLS]` 토큰과 이미지 패치들이 깨끗해져서 **성능과 시각화(Attention Map) 품질이 확 좋아졌습니다.**
        

---

### 4. 시나리오 D: 멀티 태스크 학습 (Multi-task Learning)

하나의 이미지로 여러 가지를 동시에 맞혀야 할 때 씁니다.

- **`[CLS_Color]`:** 이미지의 색상을 맞히는 헤드와 연결.
    
- **`[CLS_Shape]`:** 이미지의 모양을 맞히는 헤드와 연결.
    
- **결과:** Attention 메커니즘에 의해 `[CLS_Color]`는 색상 정보 위주로 흡수하고, `[CLS_Shape]`는 테두리 정보 위주로 흡수하게 됩니다. **하나의 모델로 여러 전문가(Experts)를 키우는 효과**가 납니다.
    

---

### 요약

`[CLS]`를 2개 이상 쓰면 어떻게 되냐고요?

1. **그냥 쓰면:** 둘이 똑같아져서 **쓸모없습니다.**
    
2. **목적을 다르게 주면 (DeiT, Multi-task):** 서로 다른 정보를 전문적으로 수집하는 **분업화**가 일어납니다. (아주 좋음)
    
3. **그냥 방치해두면 (Registers):** 모델이 알아서 **임시 메모리나 쓰레기통**으로 활용해, 전체적인 연산 효율을 높입니다. (아주 좋음)
    

즉, **토큰을 추가하는 것은 ViT 성능을 올리는 아주 훌륭한 테크닉 중 하나**입니다. 질문하신 아이디어가 실제로 최신 논문들의 핵심 주제와 맞닿아 있습니다!