입력을 완전히 이해한 후(Encoder), 새로운 출력으로 변환(Decoder)하는 **
Seq2Seq(Sequence-to-Sequence)** 구조입니다. 
트랜스포머의 원조 논문("Attention is All You Need")이 제안한 형태입니다.

- **대표 모델:** **Original Transformer**, T5, BART
    
- **핵심 메커니즘: 두 가지 어텐션의 결합**
    
    1. **Encoder:** 입력 문장 전체를 양방향으로 훑어보고 문맥 정보를 압축(Context Vector)합니다.
        
    2. **Decoder:**
        
        - **Masked Self-Attention:** 자기가 생성하고 있는 문장 내에서 앞부분만 봅니다.
            
        - **Cross-Attention (핵심):** 디코더가 단어를 생성할 때마다, **인코더가 정리해 둔 원문 정보(Key, Value)**를 슬쩍 컨닝(참조)합니다. 즉, "지금 번역할 단어와 관련된 원문의 단어는 어디지?"를 찾습니다.
            
- **학습 목표:**
    
    - 입력 시퀀스를 주고, 타겟 시퀀스(번역문, 요약문)를 생성하도록 학습합니다.
        
- **주요 활용:**
    
    - 기계 번역 (한국어 → 영어)
        
    - 텍스트 요약 (긴 뉴스 → 3줄 요약)
        
    - 문법 교정