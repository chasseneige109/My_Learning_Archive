입력된 내용을 바탕으로 **다음에 올 단어를 예측**하며 텍스트를 생성해 나가는 구조입니다. 
현재 가장 핫한 LLM(Large Language Model)의 주류입니다.

- **대표 모델:** **GPT 시리즈**, Llama, Claude
    
- **핵심 메커니즘: 인과적 어텐션 (Causal / Masked Self-Attention)**
    
    - 철저하게 **자기회귀(Autoregressive)** 방식입니다.
        
    - 특정 시점 $t$의 토큰을 예측할 때, $t$ 이후의 미래 토큰들은 절대 볼 수 없도록 **마스킹(Masking)** 처리합니다. (Look-ahead Mask)
        
    - 마치 우리가 말을 할 때, 아직 뱉지 않은 다음 단어를 미리 알 수 없는 것과 같습니다.
        
- **학습 목표:**
    
    - **NTP (Next Token Prediction):** 지금까지 나온 단어들을 보고, 바로 다음에 올 단어가 무엇일지 맞추는 방식으로 학습합니다.
        
- **주요 활용:**
    
    - 자유로운 텍스트 생성 (소설 쓰기, 이메일 작성)
        
    - 코드 생성
        
    - 챗봇 (대화형 AI)