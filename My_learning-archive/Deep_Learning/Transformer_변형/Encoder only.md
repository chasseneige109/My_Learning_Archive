
텍스트를 생성하기보다는, 입력된 텍스트의 **문맥을 깊이 이해하고 분석**하는 데 특화된 구조입니다.

- **대표 모델:** **BERT**, RoBERTa, ELECTRA
    
- **핵심 메커니즘: 양방향 어텐션 (Bidirectional Attention)**
    
    - 마스크(Mask)를 사용하지 않습니다.
        
    - 문장 내의 특정 단어를 볼 때, 그 단어의 **앞(Past)**과 **뒤(Future)**에 있는 모든 단어를 동시에 참고합니다.
        
    - 예: "The **bank** of the river."라는 문장에서 `bank`를 이해할 때, 뒤에 나오는 `river`를 볼 수 있으므로 이것이 '은행'이 아니라 '강둑'임을 즉시 파악합니다.
        
- **학습 목표:**
    
    - **MLM (Masked Language Modeling):** 문장 중간에 구멍을 뚫고(`[MASK]`), 주변 문맥(앞뒤)을 보고 빈칸을 맞추는 방식으로 학습합니다.
        
- **주요 활용:**
    
    - 텍스트 분류 (감정 분석, 스팸 필터링)
        
    - 개체명 인식 (NER: 사람 이름, 장소 등 추출)
        
    - 질의응답 (QA)에서 정답 위치 찾기
        
    - 임베딩 추출 (문장 의미를 벡터로 변환)