대표적으로 **BERT** 

--- 

### **1단계: 데이터 준비 및 전처리 (Data Preparation)**

모델에게 먹일 데이터를 요리하는 과정입니다.

1. **코퍼스(Corpus) 수집:**
    
    - 위키피디아(25억 단어) + BookCorpus(8억 단어) 같은 거대한 텍스트 데이터를 준비합니다.
        
2. **토크나이저 학습 (Vocabulary 구축):**
    
    - 준비된 텍스트로 WordPiece 같은 서브워드 토크나이저를 먼저 학습시킵니다. (예: 30,000개의 단어 조각 사전 구축)
        
3. **입력 포맷팅 (Input Formatting):**
    
    - 텍스트를 토큰 ID로 변환하고, **특수 토큰**을 붙입니다.
        
    - `[CLS] 문장 A [SEP] 문장 B [SEP]` 형태로 만듭니다.
        
    - 최대 길이(예: 512)에 맞춰 자르거나, 짧으면 `[PAD]`를 채웁니다.
        

---

### **2단계: 입력 임베딩 (Input Representation)**

숫자로 변환된 입력을 3개 각각의 임베딩 행렬을 곱해 모델 내부의 벡터로 바꿉니다. 앞서 설명한 **3가지 임베딩의 합**이 일어납니다.

1. **Token Embedding:** 각 단어 ID(`dog`, `run`)에 해당하는 벡터.
    
2. **Segment Embedding:** 문장 A 구간은 0, 문장 B 구간은 1에 해당하는 벡터.
    
3. **Position Embedding:** 첫 번째 위치, 두 번째 위치... 각 순서에 해당하는 벡터.
    
4. **최종 입력:** 위 3개 벡터를 요소별로 더함(Element-wise Sum) + Layer Normalization.
    

---

### **3단계: 사전 학습 (Pre-training) - 핵심 단계**

이 단계가 가장 중요하고 비용이 많이 듭니다. 레이블이 없는 텍스트만으로 **자기지도학습(Self-Supervised Learning)**을 수행합니다. 두 가지 미션을 동시에 수행합니다.

#### **미션 A: 마스크 언어 모델링 (MLM)**

"빈칸 채우기"를 통해 문맥을 양방향으로 이해합니다.

1. **마스킹 전략:** 입력 토큰의 **15%**를 랜덤하게 선택합니다.
    
2. **변조 (Corruption):** 선택된 15%의 토큰을 다음과 같이 바꿉니다.
    
    - **80%:** `[MASK]` 토큰으로 교체 (예: `my dog` $\rightarrow$ `my [MASK]`)
        
    - **10%:** 엉뚱한 랜덤 단어로 교체 (예: `my dog` $\rightarrow$ `my apple`) $\rightarrow$ 문맥상 틀린 단어도 고칠 수 있게 학습.
        
    - **10%:** 원래 단어 그대로 둠 (예: `my dog` $\rightarrow$ `my dog`) $\rightarrow$ 실제 데이터엔 `[MASK]`가 없으므로, 편향을 줄이기 위함.
        
3. **예측:** 모델은 `[MASK]` 위치의 출력 벡터를 가져와, 원래 단어가 무엇이었는지 전체 단어 집합(Vocabulary) 중에서 맞춥니다.
    

#### **미션 B: 다음 문장 예측 (NSP)**

문장 간의 논리적 연결성을 학습합니다.

1. **데이터 구성:** 학습 데이터의 50%는 실제 이어지는 문장(IsNext), 50%는 랜덤하게 뽑은 관계없는 문장(NotNext)으로 구성합니다.
    
2. **예측:** 맨 앞의 `[CLS]` 토큰의 출력 벡터를 사용하여, 두 문장이 이어지는지(True/False) 이진 분류를 수행합니다.
    

---

### **4단계: 손실 계산 및 역전파 (Loss & Backpropagation)**

모델이 문제를 풀었으니, 얼마나 틀렸는지 채점하고 수정합니다.

1. **전파 (Forward Pass):** 입력을 넣고 Transformer Encoder 층(12개~24개)을 통과시켜 최종 벡터들을 얻습니다.
    
2. **손실 계산 (Calculate Loss):**
    
    - **MLM Loss:** `[MASK]` 처리된 부분에서만 원래 단어와의 차이(Cross-Entropy)를 계산합니다. (나머지 85% 단어는 계산 안 함)
        
    - **NSP Loss:** `[CLS]` 토큰에서 정답(IsNext/NotNext)과의 차이를 계산합니다.
        
    - **Total Loss = MLM Loss + NSP Loss**
        
3. **역전파 (Backpropagation):** Total Loss를 줄이는 방향으로 모델의 모든 파라미터(가중치)를 미세하게 업데이트합니다.
    

이 과정을 수백만 스텝 반복하면, 모델은 언어의 문법, 문맥, 상식 등을 거대한 행렬 안에 저장하게 됩니다. 이를 **Pre-trained Model**이라고 합니다.

---

### **5단계: 파인 튜닝 (Fine-tuning)**

이제 똑똑해진 모델을 특정 작업(감정 분석, QA 등)에 맞게 살짝 재교육시킵니다.

1. **헤드 부착 (Add Task Head):**
    
    - Pre-trained 모델의 마지막 층 위에, 풀고자 하는 문제에 맞는 **가벼운 신경망(Linear Layer)**을 하나 얹습니다.
        
    - 예: 감정 분석이라면 `[CLS]` 벡터 뒤에 `분류기(긍정/부정)`를 부착.
        
2. **데이터 주입:**
    
    - 이번에는 **정답 레이블이 있는** 소량의 데이터(예: 영화 리뷰 + 평점)를 입력합니다.
        
3. **전체 업데이트:**
    
    - 새로운 데이터로 학습을 진행합니다. 이때, 새로 붙인 헤드뿐만 아니라 **기존 Pre-trained 모델의 가중치도 미세하게 같이 업데이트**합니다.
        
    - 사전 학습보다 훨씬 적은 데이터와 시간(몇 시간~몇 십 분)으로도 매우 높은 성능을 냅니다.
        

---

### **요약: 전체 시퀀스**

1. **Raw Text** $\rightarrow$ `[MASK]` 씌우고 문장 2개 붙임 **(전처리)**
    
2. **Embeddings** $\rightarrow$ 벡터 변환 **(입력)**
    
3. **Encoder Layers** $\rightarrow$ Self-Attention으로 문맥 비비기 **(연산)**
    
4. **Output Heads** $\rightarrow$ 빈칸 맞추기(MLM) & 연결 확인(NSP) **(예측)**
    
5. **Update** $\rightarrow$ 틀린 만큼 똑똑해짐 **(사전 학습 완료)**
    
6. **Task Data** $\rightarrow$ 내 문제에 맞게 최종 튜닝 **(파인 튜닝)**