### 1. 핵심 철학: "양방향(Bidirectional) 문맥 파악"

- **Decoder (GPT):** "나는 밥을..." 다음에는? (왼쪽에서 오른쪽으로만 봄, **Left-to-Right**)
    
- **Encoder (BERT):** "어제 [MASK] 먹었어."의 빈칸은? (문장의 앞과 뒤를 동시에 봄, **Bidirectional**)
    

Self-Attention 메커니즘에서 **마스킹(Look-ahead Mask)을 제거**했습니다. 즉, 자기 자신을 포함해 문장 내 모든 단어가 서로를 '참조(Attend)'할 수 있습니다.

---

### 2. 입력 구조 (Input Representation)

Encoder-only 모델은 단순히 단어만 넣는 것이 아니라, 문장의 구조를 알려주기 위해 3가지 임베딩을 합쳐서 입력합니다.

1. **Token Embeddings:** 단어 자체의 의미 (WordPiece 등 서브워드 ID).
    
2. **Segment Embeddings:** 두 문장이 들어올 때, 앞 문장(A)인지 뒷 문장(B)인지 구분하는 ID (0 또는 1).
    
3. **Position Embeddings:** 단어의 순서(위치) 정보. (트랜스포머는 병렬 처리하므로 순서 정보가 필수).
    

> **특수 토큰의 역할:**
> 
> - `[CLS]`: 문장의 맨 앞에 무조건 붙습니다. 나중에 문장 전체의 의미를 대표하는 벡터가 됩니다. (Classification 용도)
>     
> - `[SEP]`: 문장과 문장을 구분하는 구분자입니다.
>     

---

### 3. 사전 학습 (Pre-training): 어떻게 똑똑해지는가?

레이블이 없는 방대한 텍스트(위키피디아, 책 등)로 **두 가지 미션**을 수행하며 스스로 학습합니다.

#### ① MLM (Masked Language Modeling) - "빈칸 채우기"

입력 문장의 단어 중 약 **15%를 랜덤하게 가립니다(`[MASK]`).** 모델은 주변 문맥을 보고 이 가려진 단어가 원래 무엇이었는지 맞혀야 합니다.

- **입력:** `The man went to the [MASK] to buy some milk.`
    
- **추론:** `buy`, `milk`라는 주변 단어(Context)를 양방향으로 참고합니다.
    
- **정답:** `store` (또는 `supermarket`)
    
- **의미:** 단어의 동시 등장 확률과 문맥적 의미를 깊게 학습하게 됩니다.
    

#### ② NSP (Next Sentence Prediction) - "문장 관계 파악"

두 문장 A와 B를 주고, **B가 A 바로 뒤에 이어지는 문장이 맞는지(IsNext / NotNext)**를 맞춥니다.

- **입력:**
    
    - A: "비가 너무 많이 온다."
        
    - B: "그래서 우산을 챙겼다."
        
- **판단:** True (이어짐)
    
- **의미:** 문장 간의 논리적 연결 관계를 학습합니다. (QA나 자연어 추론 태스크에 도움)
    

---

### 4. 파인 튜닝 (Fine-tuning): 실전 적용

사전 학습이 끝난 Encoder 모델은 '언어의 문맥을 이해하는 거대한 두뇌'가 되었습니다. 이제 특정 작업(Task)에 맞게 **출력층(Output Layer) 하나만 추가**해서 살짝 재학습(Fine-tuning)합니다.

#### ① 텍스트 분류 (Classification)

- **활용:** 감정 분석, 스팸 분류
    
- **방법:** 문장 맨 앞의 **`[CLS]` 토큰의 출력 벡터** 하나만 꺼냅니다. 이 벡터는 문장 전체의 압축된 정보를 담고 있습니다. 여기에 분류기(Linear Layer)를 붙여 긍정/부정을 판단합니다.
    

#### ② 개체명 인식 (Named Entity Recognition - NER)

- **활용:** 문장에서 사람 이름, 날짜, 장소 찾기
    
- **방법:** **모든 토큰의 출력 벡터**를 각각 사용합니다. 각 토큰 벡터 위에 분류기를 붙여 "이 토큰이 사람인가? 장소인가? 아닌가?"를 각각 판별합니다.
    

#### ③ 질의 응답 (Question Answering - QA)

- **활용:** 지문 주고 정답 찾기 (SQuAD 등)
    
- **방법:** 질문과 지문을 `[SEP]`으로 이어 붙여 입력합니다. 지문 영역의 토큰들 중에서 **정답이 시작되는 위치(Start Index)**와 **끝나는 위치(End Index)**를 예측하도록 학습합니다.
    

---

### 5. 한계점

**생성(Generation) 능력이 부족합니다.** Encoder-only 모델은 입력 전체를 한 번에 보고 구조를 파악하는 데는 천재적이지만, 단어를 하나씩 순서대로 내뱉는(말하기) 능력은 없습니다. 문장을 생성하려면 마스킹된 토큰을 하나씩 예측하며 채워나가야 하는데, 이는 매우 비효율적이고 자연스럽지 않습니다. (그래서 생성은 GPT 같은 Decoder 모델이 담당합니다.)

### 요약

- **구조:** 양방향 어텐션 (Bidirectional Attention).
    
- **입력:** 토큰 + 세그먼트 + 포지션 임베딩.
    
- **학습법:** 빈칸 맞추기(MLM) + 다음 문장 맞추기(NSP).
    
- **결과물:** 텍스트의 깊은 의미가 담긴 **고차원 벡터(Contextualized Word Vectors).**
    
- **주특기:** 분류, 추출, 문맥 비교 (생성 제외 모든 것).