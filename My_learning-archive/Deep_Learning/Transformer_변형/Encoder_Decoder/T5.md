# **Text-to-Text Transfer Transformer 

- BERT(Encoder)와 GPT(Decoder)를 결합

- "Encoder가 소화시킨 정보를 Decoder가 어떻게 가져다 쓰는가?" 
--> Cross Attention


# A. T5의 경우 (Span Corruption)

T5는 모든 문제를 **"텍스트 $\rightarrow$ 텍스트"**로 변환하는 단일 프레임워크를 사용합니다.

- **학습 목표:** 입력 텍스트의 여러 부분을 `[MASK]` 대신 **`[SENTINEL]`** 토큰으로 가리고, Encoder가 이를 이해한 후 Decoder가 가려진 스팬(Span)들을 복구하도록 합니다.
    
    - **입력:** `The man [SENTINEL 1] went to [SENTINEL 2].`
        
    - **정답:** `[SENTINEL 1] bought a sandwich [SENTINEL 2] the store.`
        
- **NSP 대체 효과:** 문장의 연속된 부분을 복구하는 과정에서 문맥적 이해와 흐름 파악 능력을 동시에 학습하게 되므로, NSP의 목표가 자연스럽게 포함됩니다.