
### 1. 기존 방식의 한계 (Word vs. Character)

서브워드의 필요성을 이해하려면 양극단의 단점을 먼저 봐야 합니다.

|**구분**|**방식**|**문제점**|
|---|---|---|
|**단어 단위**|띄어쓰기 기준 분리<br><br>  <br><br>(예: `apple`, `apples`)|**1. OOV (Out-Of-Vocabulary) 문제:** 학습 때 본 적 없는 단어(희귀 단어, 오타)가 나오면 `[UNK]`(Unknown) 처리되어 정보를 잃음.<br><br>  <br><br>**2. 단어 집합이 너무 큼:** `work`, `works`, `worked`를 모두 다른 단어로 저장해야 해서 메모리 낭비가 심함.|
|**문자 단위**|글자 하나하나 분리<br><br>  <br><br>(예: `a`, `p`, `p`, `l`, `e`)|**1. 의미 파악 어려움:** 'a'라는 글자 자체에는 의미가 없음.<br><br>  <br><br>**2. 시퀀스가 너무 길어짐:** 문장이 너무 길어져서 모델이 문맥을 학습하기 어려움.|

---

### 2. 서브워드(Subword)의 해결책: "의미 있는 조각으로 쪼개자"

서브워드 방식(BPE, WordPiece 등)은 **"빈도수가 높은(자주 쓰이는) 문자 조합은 하나의 토큰으로 묶고, 희귀한 단어는 쪼개자"**는 전략을 취합니다.

#### ① 미등록 단어(OOV)와 희귀 단어 처리

단어 단위 토크나이저가 'chatbot'이라는 단어를 학습하지 않았다면, 이를 모르는 단어(`[UNK]`)로 취급합니다. 하지만 서브워드 방식은 이를 아는 단어들의 조합으로 분해합니다.

- **예시:** `chatbot`이라는 단어가 사전에 없을 때
    
    - **분해:** `chat` (자주 등장) + `bot` (자주 등장)
        
    - **결과:** 모델은 '대화(chat)'와 '로봇(bot)'의 의미를 결합하여 `chatbot`의 의미를 유추할 수 있습니다.
        
- **효과:** 처음 보는 신조어나 복합어가 나와도, 기존에 학습된 서브워드들의 조합으로 표현이 가능하므로 **OOV 문제가 획기적으로 줄어듭니다.**

#### ② 다양한 변형 및 어미 처리 (Morphology)

`work`, `worked`, `working`과 같은 변형을 처리하는 방식이 매우 효율적입니다.

- **영어의 예 (굴절어):**
    
    - `working` → `work` (어근) + `##ing` (접미사)
        
    - `worked` → `work` (어근) + `##ed` (접미사)
        
    - **효과:** 모델은 `work`라는 공통된 동작의 의미를 학습하고, `ing`(진행), `ed`(과거)라는 문법적 기능을 별도로 학습하여 조합합니다. 즉, 단어 3개를 따로 외우는 게 아니라 **'어근 1개 + 접미사 2개'로 수만 가지 조합을 커버**합니다.
        
- **한국어의 예 (교착어 - 매우 중요):** 한국어는 어간에 조사나 어미가 붙는 교착어이므로 서브워드 방식이 더욱 강력합니다.
    
    - 입력: `먹었습니다`
        
    - 단어 단위: `먹었습니다`를 통째로 외워야 함. (`먹었고`, `먹어서` 등 무한한 변형 존재)
        
    - **서브워드 단위:** `먹` (어간) + `었` (과거 시제) + `습니다` (종결 어미)
        
    - **효과:** 모델은 '먹다'라는 행위와 시제, 높임법을 각각의 토큰으로 인식하여 문맥을 훨씬 정교하게 파악합니다.
        

#### ③ 오탈자 강건성 (Robustness)

사용자가 오타를 내더라도 서브워드 방식은 의미를 어느 정도 복구할 수 있습니다.

- 입력: `helo` (hello의 오타)
    
- 분해: `he` + `lo` (비록 완벽한 `hello`는 아니지만, 유사한 발음이나 철자 조합을 통해 모델이 문맥상 인사말임을 유추할 확률이 높아짐)